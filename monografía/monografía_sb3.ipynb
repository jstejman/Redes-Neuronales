{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05820b17",
   "metadata": {
    "papermill": {
     "duration": 0.004748,
     "end_time": "2026-02-21T23:31:44.237302",
     "exception": false,
     "start_time": "2026-02-21T23:31:44.232554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Monograf√≠a Final\n",
    "## Redes Neuronales - FIUBA\n",
    "### Alumno: Juli√°n Stejman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc18a4",
   "metadata": {
    "papermill": {
     "duration": 0.003663,
     "end_time": "2026-02-21T23:31:44.245430",
     "exception": false,
     "start_time": "2026-02-21T23:31:44.241767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "En este documento se utilizar√° una red neuronal entrenada con un algoritmo de aprendizaje por refuerzo PPO (Proximal Policy Optimization) que aprendiese a jugar niveles de Super Mario Bros para la NES. Para ello, se utilizar√° el entorno de Gymnasium (sucesor de OpenAI Gym), que simula el juego de Super Mario Bros, junto con una arquitectura Actor-Critic basada en capas convolucionales y frame stacking para capturar informaci√≥n temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca8066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T23:31:44.254303Z",
     "iopub.status.busy": "2026-02-21T23:31:44.254050Z",
     "iopub.status.idle": "2026-02-21T23:32:01.738295Z",
     "shell.execute_reply": "2026-02-21T23:32:01.737589Z"
    },
    "papermill": {
     "duration": 17.490412,
     "end_time": "2026-02-21T23:32:01.740100",
     "exception": false,
     "start_time": "2026-02-21T23:31:44.249688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym_super_mario_bros in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (7.4.0)\n",
      "Requirement already satisfied: torch in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (2.10.0)\n",
      "Requirement already satisfied: nes-py>=8.1.4 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from gym_super_mario_bros) (8.2.1)\n",
      "Requirement already satisfied: filelock in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (3.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (82.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (2026.2.0)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from cuda-bindings==12.9.4->torch) (1.3.4)\n",
      "Requirement already satisfied: gym>=0.17.2 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (2.4.2)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (1.5.21)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (4.67.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym_super_mario_bros) (3.1.2)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym_super_mario_bros) (0.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages (from jinja2->torch) (3.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym_super_mario_bros torch gymnasium \"shimmy[gym-v26]\" stable-baselines3\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "import gymnasium\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ccb235d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T23:32:01.751086Z",
     "iopub.status.busy": "2026-02-21T23:32:01.750770Z",
     "iopub.status.idle": "2026-02-21T23:32:01.754913Z",
     "shell.execute_reply": "2026-02-21T23:32:01.754026Z"
    },
    "papermill": {
     "duration": 0.010987,
     "end_time": "2026-02-21T23:32:01.756371",
     "exception": false,
     "start_time": "2026-02-21T23:32:01.745384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3062412c",
   "metadata": {
    "papermill": {
     "duration": 0.00452,
     "end_time": "2026-02-21T23:32:01.766000",
     "exception": false,
     "start_time": "2026-02-21T23:32:01.761480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Si se eliminara en la siguiente celda la l√≠nea que dice %%skip, se podr√≠a ver una simulaci√≥n simple de como funciona el emulador del juego con el entorno. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff39d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T23:32:01.776275Z",
     "iopub.status.busy": "2026-02-21T23:32:01.776068Z",
     "iopub.status.idle": "2026-02-21T23:32:01.779558Z",
     "shell.execute_reply": "2026-02-21T23:32:01.778817Z"
    },
    "papermill": {
     "duration": 0.01001,
     "end_time": "2026-02-21T23:32:01.780952",
     "exception": false,
     "start_time": "2026-02-21T23:32:01.770942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "%%skip\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from shimmy import GymV26CompatibilityV0\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "env = GymV26CompatibilityV0(env)\n",
    "\n",
    "done = True\n",
    "for step in range(5000):\n",
    "    if done:\n",
    "        state, info = env.reset()\n",
    "    state, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    done = terminated or truncated\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f731c832",
   "metadata": {
    "papermill": {
     "duration": 0.004697,
     "end_time": "2026-02-21T23:32:01.790435",
     "exception": false,
     "start_time": "2026-02-21T23:32:01.785738",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "La idea ser√≠a armar una red neuronal convolucional para poder interpretar de las im√°genes recibidas una posible acci√≥n para que tomara el personaje del juego. Esta red luego utilizar√° un algoritmo de aprendizaje por refuerzo para poder mejorar su desempe√±o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f28e4",
   "metadata": {
    "papermill": {
     "duration": 0.004627,
     "end_time": "2026-02-21T23:32:01.799629",
     "exception": false,
     "start_time": "2026-02-21T23:32:01.795002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "La arquitectura utiliza una red Actor-Critic con capas convolucionales que procesan 4 frames apilados del juego. Al recibir m√∫ltiples frames consecutivos, la CNN puede inferir velocidad y direcci√≥n del movimiento sin necesidad de una capa recurrente. El Actor predice la distribuci√≥n de probabilidades sobre acciones (policy), mientras que el Critic estima el valor esperado del estado para el c√°lculo del advantage. Esta es la misma arquitectura que utiliza internamente Stable Baselines3 con `CnnPolicy`, y la que DeepMind us√≥ originalmente para Atari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c049ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T23:32:01.810116Z",
     "iopub.status.busy": "2026-02-21T23:32:01.809915Z",
     "iopub.status.idle": "2026-02-21T23:32:01.817577Z",
     "shell.execute_reply": "2026-02-21T23:32:01.816939Z"
    },
    "papermill": {
     "duration": 0.014181,
     "end_time": "2026-02-21T23:32:01.818865",
     "exception": false,
     "start_time": "2026-02-21T23:32:01.804684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PPOActorCritic(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions, hidden_size=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Capas convolucionales para procesamiento de im√°genes\n",
    "        # Arquitectura est√°ndar Nature DQN (Mnih et al., 2015)\n",
    "        self.image_processing = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Calcular tama√±o de salida de las conv\n",
    "        flatten_size = self._get_flatten_size(input_shape)\n",
    "        \n",
    "        # Feature extractor compartido\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(flatten_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor: predice distribuci√≥n sobre acciones\n",
    "        self.actor = nn.Linear(hidden_size, num_actions)\n",
    "        \n",
    "        # Critic: estima valor del estado\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Inicializaci√≥n ortogonal (mejora estabilidad del entrenamiento)\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _get_flatten_size(self, input_shape):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)\n",
    "            output = self.image_processing(dummy_input)\n",
    "            return output.shape[1]\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Inicializaci√≥n ortogonal est√°ndar para PPO\"\"\"\n",
    "        for module in self.image_processing:\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.orthogonal_(module.weight, gain=nn.init.calculate_gain('relu'))\n",
    "                nn.init.zeros_(module.bias)\n",
    "        nn.init.orthogonal_(self.shared_fc[0].weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(self.shared_fc[0].bias)\n",
    "        nn.init.orthogonal_(self.actor.weight, gain=0.01)  # Peque√±o para pol√≠tica inicial uniforme\n",
    "        nn.init.zeros_(self.actor.bias)\n",
    "        nn.init.orthogonal_(self.critic.weight, gain=1.0)\n",
    "        nn.init.zeros_(self.critic.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, channels, height, width) ‚Äî channels = 4 frames apilados\n",
    "        \"\"\"\n",
    "        features = self.image_processing(x)\n",
    "        features = self.shared_fc(features)\n",
    "        \n",
    "        action_logits = self.actor(features)\n",
    "        value = self.critic(features)\n",
    "        \n",
    "        return action_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24d7868c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T23:32:01.829478Z",
     "iopub.status.busy": "2026-02-21T23:32:01.829186Z",
     "iopub.status.idle": "2026-02-21T23:32:02.189551Z",
     "shell.execute_reply": "2026-02-21T23:32:02.188390Z"
    },
    "papermill": {
     "duration": 0.367024,
     "end_time": "2026-02-21T23:32:02.191004",
     "exception": false,
     "start_time": "2026-02-21T23:32:01.823980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action logits shape: torch.Size([1, 5])\n",
      "Value shape: torch.Size([1, 1])\n",
      "Number of actions: 5\n",
      "Total parameters: 1,687,206\n"
     ]
    }
   ],
   "source": [
    "frame_stack_count = 4\n",
    "dimensions = 84\n",
    "input_shape = (frame_stack_count, dimensions, dimensions)\n",
    "num_actions = len(RIGHT_ONLY)\n",
    "\n",
    "test_model = PPOActorCritic(input_shape, num_actions)\n",
    "dummy_state = torch.zeros(1, *input_shape)  # (batch_size, channels, height, width)\n",
    "action_logits, value = test_model(dummy_state)\n",
    "print(f\"Action logits shape: {action_logits.shape}\")\n",
    "print(f\"Value shape: {value.shape}\")\n",
    "print(f\"Number of actions: {num_actions}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in test_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2e5d7d",
   "metadata": {
    "papermill": {
     "duration": 0.004878,
     "end_time": "2026-02-21T23:32:02.200973",
     "exception": false,
     "start_time": "2026-02-21T23:32:02.196095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Para enviar a la red, se toman las siguientes decisiones de preprocesamiento est√°ndar en RL:\n",
    "\n",
    "1. **Frame skip (4)**: Repetir cada acci√≥n durante 4 frames consecutivos. Esto reduce la cantidad de decisiones que debe tomar el agente y hace el entrenamiento ~4x m√°s r√°pido.\n",
    "2. **Escala de grises**: Convertir de RGB a blanco y negro (reduce de 3 canales a 1).\n",
    "3. **Resoluci√≥n 84√ó84**: Tama√±o est√°ndar para RL en juegos, suficiente para capturar la informaci√≥n visual relevante.\n",
    "4. **Frame stacking (4)**: Apilar los √∫ltimos 4 frames como canales, resultando en un tensor `(4, 84, 84)`. Esto permite que la CNN pueda inferir velocidad y direcci√≥n del movimiento.\n",
    "\n",
    "Todo esto se implementa como wrappers del entorno, siguiendo la convenci√≥n de Gymnasium con `shimmy` como bridge para el entorno legacy de `gym_super_mario_bros`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c38a72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T23:32:02.211499Z",
     "iopub.status.busy": "2026-02-21T23:32:02.211214Z",
     "iopub.status.idle": "2026-02-21T23:32:02.964552Z",
     "shell.execute_reply": "2026-02-21T23:32:02.963687Z"
    },
    "papermill": {
     "duration": 0.760386,
     "end_time": "2026-02-21T23:32:02.966062",
     "exception": false,
     "start_time": "2026-02-21T23:32:02.205676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/.pyenv/versions/3.14.3/lib/python3.14/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "Python integer 1024 out of bounds for uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOverflowError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Crear entorno y verificar\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m env = \u001b[43mmake_mario_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m state = env.reset()\n\u001b[32m     77\u001b[39m state_np = np.array(state)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mmake_mario_env\u001b[39m\u001b[34m(world, stage)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_mario_env\u001b[39m(world=\u001b[32m1\u001b[39m, stage=\u001b[32m1\u001b[39m):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Crea el entorno de Mario con todos los wrappers est√°ndar de RL.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     env = \u001b[43mgym_super_mario_bros\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSuperMarioBros-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mworld\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m-v0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     env = JoypadSpace(env, RIGHT_ONLY)\n\u001b[32m     66\u001b[39m     env = CompatResetWrapper(env)  \u001b[38;5;66;03m# Absorbe seed/options para SB3\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.14.3/lib/python3.14/site-packages/gym/envs/registration.py:640\u001b[39m, in \u001b[36mmake\u001b[39m\u001b[34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[39m\n\u001b[32m    637\u001b[39m     render_mode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     env = \u001b[43menv_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    642\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    643\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e).find(\u001b[33m\"\u001b[39m\u001b[33mgot an unexpected keyword argument \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrender_mode\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m) >= \u001b[32m0\u001b[39m\n\u001b[32m    644\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m apply_human_rendering\n\u001b[32m    645\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.14.3/lib/python3.14/site-packages/gym_super_mario_bros/smb_env.py:52\u001b[39m, in \u001b[36mSuperMarioBrosEnv.__init__\u001b[39m\u001b[34m(self, rom_mode, lost_levels, target)\u001b[39m\n\u001b[32m     50\u001b[39m rom = rom_path(lost_levels, rom_mode)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# initialize the super object with the ROM path\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSuperMarioBrosEnv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrom\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# set the target world, stage, and area variables\u001b[39;00m\n\u001b[32m     54\u001b[39m target = decode_target(target, lost_levels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.14.3/lib/python3.14/site-packages/nes_py/nes_env.py:126\u001b[39m, in \u001b[36mNESEnv.__init__\u001b[39m\u001b[34m(self, rom_path)\u001b[39m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mROM has trainer. trainer is not supported.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# try to read the PRG ROM and raise a value error if it fails\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m _ = \u001b[43mrom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprg_rom\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# try to read the CHR ROM and raise a value error if it fails\u001b[39;00m\n\u001b[32m    128\u001b[39m _ = rom.chr_rom\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.14.3/lib/python3.14/site-packages/nes_py/_rom.py:204\u001b[39m, in \u001b[36mROM.prg_rom\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the PRG ROM of the ROM file.\"\"\"\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw_data[\u001b[38;5;28mself\u001b[39m.prg_rom_start:\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprg_rom_stop\u001b[49m]\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mfailed to read PRG-ROM on ROM.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.14.3/lib/python3.14/site-packages/nes_py/_rom.py:198\u001b[39m, in \u001b[36mROM.prg_rom_stop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprg_rom_stop\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    197\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"The exclusive stopping index of the PRG ROM.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prg_rom_start + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprg_rom_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m10\u001b[39;49m\n",
      "\u001b[31mOverflowError\u001b[39m: Python integer 1024 out of bounds for uint8"
     ]
    }
   ],
   "source": [
    "import gymnasium\n",
    "import numpy as np\n",
    "from gymnasium import Wrapper, ObservationWrapper\n",
    "from gymnasium.wrappers import GrayscaleObservation, ResizeObservation, FrameStackObservation\n",
    "from gymnasium import spaces\n",
    "from shimmy import GymV26CompatibilityV0\n",
    "\n",
    "class SkipFrame(Wrapper):\n",
    "    \"\"\"\n",
    "    Repite la misma acci√≥n durante `skip` frames y acumula la recompensa.\n",
    "    \n",
    "    Esto es est√°ndar en RL para Atari/Mario porque:\n",
    "    - Frames consecutivos son casi id√©nticos, no tiene sentido decidir cada uno\n",
    "    - Reduce 4x la cantidad de decisiones ‚Üí entrenamiento m√°s r√°pido\n",
    "    - Reduce ruido en la se√±al de reward\n",
    "    \"\"\"\n",
    "    def __init__(self, env, skip=4):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "    \n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        for _ in range(self._skip):\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        return state, total_reward, terminated, truncated, info\n",
    "\n",
    "class SqueezeObservation(ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Elimina la dimensi√≥n de canal trailing si es 1: (84,84,1) ‚Üí (84,84).\n",
    "    \n",
    "    Necesario porque algunas versiones de gymnasium producen (H,W,1) despu√©s de\n",
    "    GrayscaleObservation + ResizeObservation, y FrameStackObservation lo apilar√≠a como\n",
    "    (4,84,84,1) en vez de (4,84,84).\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs = self.observation_space\n",
    "        if len(obs.shape) == 3 and obs.shape[-1] == 1:\n",
    "            self.observation_space = spaces.Box(\n",
    "                low=obs.low.squeeze(-1), high=obs.high.squeeze(-1), dtype=obs.dtype\n",
    "            )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        if obs.ndim == 3 and obs.shape[-1] == 1:\n",
    "            return obs.squeeze(-1)\n",
    "        return obs\n",
    "\n",
    "def make_mario_env(world=1, stage=1):\n",
    "    \"\"\"Crea el entorno de Mario con todos los wrappers est√°ndar de RL.\"\"\"\n",
    "    env = gym_super_mario_bros.make(f'SuperMarioBros-{world}-{stage}-v0')\n",
    "    env = JoypadSpace(env, RIGHT_ONLY)\n",
    "    env = GymV26CompatibilityV0(env)                                    # Bridge gym ‚Üí gymnasium\n",
    "    env = SkipFrame(env, skip=4)\n",
    "    env = ResizeObservation(env, shape=(dimensions, dimensions))        # (84, 84, 3)\n",
    "    env = GrayscaleObservation(env, keep_dims=False)                    # (84, 84)\n",
    "    env = SqueezeObservation(env)                                       # (84, 84) garantizado\n",
    "    env = FrameStackObservation(env, stack_size=frame_stack_count)      # (4, 84, 84)\n",
    "    return env\n",
    "\n",
    "# Crear entorno y verificar\n",
    "env = make_mario_env()\n",
    "state, info = env.reset()\n",
    "state_np = np.array(state)\n",
    "print(f\"State shape: {state_np.shape}  ‚Üê debe ser (4, 84, 84)\")\n",
    "print(f\"Action space: {env.action_space.n} acciones: {RIGHT_ONLY}\")\n",
    "assert state_np.shape == (4, 84, 84), f\"Shape incorrecto: {state_np.shape}\"\n",
    "\n",
    "# Visualizar los 4 frames apilados\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i in range(4):\n",
    "    axes[i].imshow(state_np[i], cmap='gray')\n",
    "    axes[i].set_title(f'Frame {i+1}')\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle('4 frames apilados (estado inicial)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2165a0c3",
   "metadata": {
    "papermill": {
     "duration": 0.005738,
     "end_time": "2026-02-21T23:32:02.978675",
     "exception": false,
     "start_time": "2026-02-21T23:32:02.972937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Para el entrenamiento con PPO, se utiliza un rollout buffer que acumula experiencias de un episodio completo o de N pasos de interacci√≥n. A diferencia de DQN (off-policy), PPO es on-policy: utiliza las experiencias para actualizar la red y luego las descarta, enfoc√°ndose en lo m√°s reciente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df71735",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T23:32:02.991089Z",
     "iopub.status.busy": "2026-02-21T23:32:02.990862Z",
     "iopub.status.idle": "2026-02-21T23:32:02.997913Z",
     "shell.execute_reply": "2026-02-21T23:32:02.997303Z"
    },
    "papermill": {
     "duration": 0.014723,
     "end_time": "2026-02-21T23:32:02.999109",
     "exception": false,
     "start_time": "2026-02-21T23:32:02.984386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class RolloutBuffer:\n",
    "    \"\"\"\n",
    "    Buffer on-policy para PPO: almacena experiencias de interacci√≥n con el entorno.\n",
    "    \n",
    "    A diferencia de DQN (off-policy), PPO es on-policy: recolecta experiencias, \n",
    "    las usa para actualizar la red, y luego las descarta.\n",
    "    \n",
    "    El buffer almacena:\n",
    "    - states: observaciones del entorno (4 frames apilados)\n",
    "    - actions: acciones tomadas\n",
    "    - rewards: recompensas recibidas\n",
    "    - values: estimaciones V(s) del critic\n",
    "    - log_probs: log-probabilidades de las acciones bajo la pol√≠tica actual\n",
    "    - dones: flags de fin de episodio\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "    \n",
    "    def push(self, state, action, reward, value, log_prob, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def compute_returns_and_advantages(self, last_value=0, gamma=0.99, gae_lambda=0.95):\n",
    "        \"\"\"\n",
    "        Calcula returns y advantages usando Generalized Advantage Estimation (GAE).\n",
    "        \n",
    "        GAE balancea entre TD (bajo bias, alta varianza) y Monte Carlo (alto bias, baja varianza).\n",
    "        Con Œª=0.95 se obtiene un buen compromiso.\n",
    "        \n",
    "        F√≥rmulas:\n",
    "        - Œ¥_t = r_t + Œ≥ * V(s_{t+1}) - V(s_t)\n",
    "        - A_t = Œ£_{l=0}^{‚àû} (Œ≥Œª)^l * Œ¥_{t+l}\n",
    "        - R_t = A_t + V(s_t)\n",
    "        \"\"\"\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        # Iterar hacia atr√°s\n",
    "        for t in reversed(range(len(self.rewards))):\n",
    "            if t == len(self.rewards) - 1:\n",
    "                next_non_terminal = 1.0 - self.dones[t]\n",
    "                next_value = last_value\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - self.dones[t]\n",
    "                next_value = self.values[t + 1]\n",
    "            \n",
    "            delta = self.rewards[t] + gamma * next_value * next_non_terminal - self.values[t]\n",
    "            gae = delta + gamma * gae_lambda * next_non_terminal * gae\n",
    "            \n",
    "            returns.insert(0, gae + self.values[t])\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        return torch.tensor(returns, dtype=torch.float32), torch.tensor(advantages, dtype=torch.float32)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c66fd8b",
   "metadata": {
    "papermill": {
     "duration": 0.005382,
     "end_time": "2026-02-21T23:32:03.010192",
     "exception": false,
     "start_time": "2026-02-21T23:32:03.004810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Funci√≥n de Entrenamiento PPO\n",
    "\n",
    "### Flujo del entrenamiento:\n",
    "```\n",
    "Para cada episodio:\n",
    "  1. Recolectar experiencias (rollout)\n",
    "     - Forward pass: observaci√≥n ‚Üí CNN ‚Üí Actor & Critic\n",
    "     - Sample acci√≥n de la distribuci√≥n del Actor\n",
    "     - Guardar (state, action, reward, log_prob, value, done)\n",
    "  2. Si no termin√≥, bootstrap V(s_final) para GAE\n",
    "  3. Calcular returns y advantages con GAE (Œª=0.95)\n",
    "  4. Normalizar advantages\n",
    "  5. Para cada √©poca (4 t√≠picamente):\n",
    "     - Barajar datos en mini-batches\n",
    "     - Calcular PPO clipped loss + value loss - entrop√≠a\n",
    "     - Actualizar modelo con gradient clipping\n",
    "  6. Guardar checkpoint cada 25 episodios\n",
    "```\n",
    "\n",
    "### Detalles clave:\n",
    "- **Sin LSTM**: La CNN procesa los 4 frames apilados directamente. Esto es m√°s simple y estable que usar LSTM, ya que los 4 frames ya contienen informaci√≥n temporal suficiente.\n",
    "- **Frame skip**: El wrapper `SkipFrame` repite cada acci√≥n 4 veces, reduciendo la cantidad de decisiones y acelerando el entrenamiento.\n",
    "- **Reward shaping conservador**: Normalizamos el reward del entorno y agregamos un bonus suave por avanzar. No usamos bonuses grandes que desestabilicen el entrenamiento.\n",
    "- **Bootstrap**: Si el episodio no termin√≥ naturalmente (truncado por `max_steps`), estimamos el valor futuro con el Critic para no perder informaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5d091",
   "metadata": {
    "papermill": {
     "duration": 0.00553,
     "end_time": "2026-02-21T23:32:03.021090",
     "exception": false,
     "start_time": "2026-02-21T23:32:03.015560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocesamiento con Wrappers de Gymnasium\n",
    "\n",
    "El pipeline de preprocesamiento se implementa como una cadena de wrappers, donde cada uno transforma la observaci√≥n del entorno. Dado que `gym_super_mario_bros` usa la API legacy de `gym`, se utiliza `shimmy.GymV26CompatibilityV0` como bridge hacia Gymnasium:\n",
    "\n",
    "```\n",
    "SuperMarioBros-v0  ‚Üí  JoypadSpace  ‚Üí  Shimmy Bridge  ‚Üí  SkipFrame  ‚Üí  GrayScale  ‚Üí  Resize  ‚Üí  FrameStack\n",
    "     (240,256,3)      (reduce acciones)  (gym‚Üígymnasium)  (repite 4)   (H,W‚ÜíH,W)   (84,84)    (4,84,84)\n",
    "```\n",
    "\n",
    "### ¬øPor qu√© estos wrappers?\n",
    "\n",
    "| Wrapper | Qu√© hace | Por qu√© |\n",
    "|---------|----------|---------|\n",
    "| `JoypadSpace` | Reduce 256 acciones ‚Üí 5 (RIGHT_ONLY) | Espacio de acciones m√°s peque√±o = m√°s f√°cil de aprender |\n",
    "| `GymV26CompatibilityV0` | Convierte API gym ‚Üí gymnasium | Necesario para compatibilidad con SB3 2.x y wrappers modernos |\n",
    "| `SkipFrame(4)` | Repite acci√≥n 4 frames, acumula reward | 4x m√°s r√°pido, frames consecutivos son casi iguales |\n",
    "| `GrayscaleObservation` | RGB ‚Üí escala de grises | 3x menos datos, el color no aporta informaci√≥n √∫til |\n",
    "| `ResizeObservation(84,84)` | Redimensiona a 84√ó84 | Tama√±o est√°ndar, reduce c√≥mputo en CNN |\n",
    "| `FrameStackObservation(4)` | Apila √∫ltimos 4 frames como canales | Permite inferir velocidad y direcci√≥n |\n",
    "\n",
    "**Nota**: Todo el preprocesamiento ocurre en los wrappers del entorno, no manualmente en el loop de entrenamiento. Esto es m√°s limpio y menos propenso a errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed03cd6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T23:32:03.032860Z",
     "iopub.status.busy": "2026-02-21T23:32:03.032652Z",
     "iopub.status.idle": "2026-02-21T23:32:03.041516Z",
     "shell.execute_reply": "2026-02-21T23:32:03.040633Z"
    },
    "papermill": {
     "duration": 0.016157,
     "end_time": "2026-02-21T23:32:03.042726",
     "exception": false,
     "start_time": "2026-02-21T23:32:03.026569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funci√≥n de diagn√≥stico cargada. √ösala con: diagnostico_training(improved_rewards)\n"
     ]
    }
   ],
   "source": [
    "def diagnostico_training(rewards, window=10):\n",
    "    \"\"\"\n",
    "    An√°lisis detallado del entrenamiento para detectar problemas\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üîç DIAGN√ìSTICO DEL ENTRENAMIENTO\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # 1. Estad√≠sticas b√°sicas\n",
    "    print(f\"üìä Estad√≠sticas Generales:\")\n",
    "    print(f\"   Total episodios: {len(rewards)}\")\n",
    "    print(f\"   Reward promedio: {np.mean(rewards):.2f}\")\n",
    "    print(f\"   Reward std: {np.std(rewards):.2f}\")\n",
    "    print(f\"   Reward m√°ximo: {max(rewards):.2f}\")\n",
    "    print(f\"   Reward m√≠nimo: {min(rewards):.2f}\\n\")\n",
    "    \n",
    "    # 2. Tendencia\n",
    "    if len(rewards) >= 20:\n",
    "        first_half = rewards[:len(rewards)//2]\n",
    "        second_half = rewards[len(rewards)//2:]\n",
    "        improvement = np.mean(second_half) - np.mean(first_half)\n",
    "        \n",
    "        print(f\"üìà Tendencia:\")\n",
    "        print(f\"   Primera mitad avg: {np.mean(first_half):.2f}\")\n",
    "        print(f\"   Segunda mitad avg: {np.mean(second_half):.2f}\")\n",
    "        print(f\"   Mejora: {improvement:+.2f} \", end=\"\")\n",
    "        \n",
    "        if improvement > 50:\n",
    "            print(\"‚úÖ (Buena progresi√≥n)\")\n",
    "        elif improvement > 0:\n",
    "            print(\"‚ö†Ô∏è  (Mejora lenta)\")\n",
    "        else:\n",
    "            print(\"‚ùå (Sin mejora o empeorando)\")\n",
    "        print()\n",
    "    \n",
    "    # 3. √öltimos episodios\n",
    "    if len(rewards) >= window:\n",
    "        recent = rewards[-window:]\n",
    "        print(f\"üéØ √öltimos {window} episodios:\")\n",
    "        print(f\"   Promedio: {np.mean(recent):.2f}\")\n",
    "        print(f\"   Mejor: {max(recent):.2f}\")\n",
    "        print(f\"   Peor: {min(recent):.2f}\")\n",
    "        print(f\"   Std: {np.std(recent):.2f} \", end=\"\")\n",
    "        \n",
    "        if np.std(recent) > 200:\n",
    "            print(\"‚ö†Ô∏è  (Alta varianza - exploraci√≥n activa)\")\n",
    "        elif np.std(recent) < 10:\n",
    "            print(\"‚ùå (Muy baja varianza - colaps√≥ a pol√≠tica fija)\")\n",
    "        else:\n",
    "            print(\"‚úÖ (Varianza saludable)\")\n",
    "        print()\n",
    "    \n",
    "    # 4. Estancamiento\n",
    "    if len(rewards) >= 30:\n",
    "        last_30 = rewards[-30:]\n",
    "        moving_avg = np.convolve(last_30, np.ones(10)/10, mode='valid')\n",
    "        \n",
    "        if len(moving_avg) > 1:\n",
    "            trend = np.polyfit(range(len(moving_avg)), moving_avg, 1)[0]\n",
    "            print(f\"üìâ An√°lisis de estancamiento (√∫ltimos 30):\")\n",
    "            print(f\"   Pendiente del promedio m√≥vil: {trend:.3f}\")\n",
    "            \n",
    "            if abs(trend) < 0.5:\n",
    "                print(\"   ‚ùå ESTANCADO - Considerar:\")\n",
    "                print(\"      ‚Ä¢ Aumentar entropy_coef_start a 0.1\")\n",
    "                print(\"      ‚Ä¢ Reducir clip_ratio a 0.1\")\n",
    "                print(\"      ‚Ä¢ Modificar reward shaping\")\n",
    "                print(\"      ‚Ä¢ Reiniciar entrenamiento desde cero\")\n",
    "            elif trend > 0:\n",
    "                print(\"   ‚úÖ Mejorando gradualmente - seguir entrenando\")\n",
    "            else:\n",
    "                print(\"   ‚ö†Ô∏è  Decayendo - posible overfitting\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "\n",
    "# Ejemplo de uso (descomentar despu√©s de entrenar):\n",
    "# diagnostico_training(improved_rewards)\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de diagn√≥stico cargada. √ösala con: diagnostico_training(improved_rewards)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700ca490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T23:32:03.054883Z",
     "iopub.status.busy": "2026-02-21T23:32:03.054656Z",
     "iopub.status.idle": "2026-02-21T23:32:03.071373Z",
     "shell.execute_reply": "2026-02-21T23:32:03.070608Z"
    },
    "papermill": {
     "duration": 0.024375,
     "end_time": "2026-02-21T23:32:03.072648",
     "exception": false,
     "start_time": "2026-02-21T23:32:03.048273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_ppo(env, model, num_episodes=500, num_epochs=4,\n",
    "              batch_size=64, gamma=0.99, gae_lambda=0.95, learning_rate=2.5e-4,\n",
    "              clip_ratio=0.2, entropy_coef=0.01, value_coef=0.5,\n",
    "              device=\"cpu\", checkpoint_freq=25, checkpoint_path=\"mario_ppo.pt\",\n",
    "              max_steps_per_episode=2000):\n",
    "    \"\"\"\n",
    "    Entrenamiento PPO est√°ndar (sin LSTM).\n",
    "    \n",
    "    Mejoras sobre la versi√≥n anterior:\n",
    "    - Sin LSTM ‚Üí entrenamiento m√°s simple y estable\n",
    "    - Frame skip via wrappers ‚Üí 4x menos decisiones por episodio\n",
    "    - Normalizaci√≥n de observaciones ‚Üí entrenamiento m√°s r√°pido\n",
    "    - Reward shaping conservador ‚Üí se√±al m√°s estable\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=1e-5)\n",
    "    \n",
    "    total_rewards = []  # Reward del entorno (sin shaping)\n",
    "    best_reward = -float('inf')\n",
    "    global_step = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # ========== RECOLECCI√ìN DE EXPERIENCIAS ==========\n",
    "        state, _info = env.reset()\n",
    "        buffer = RolloutBuffer()\n",
    "        \n",
    "        episode_reward = 0       # Reward real del entorno\n",
    "        episode_shaped = 0       # Reward con shaping (para el agente)\n",
    "        episode_length = 0\n",
    "        done = False\n",
    "        prev_x_pos = 40\n",
    "        max_x_pos = 40\n",
    "        \n",
    "        while not done and episode_length < max_steps_per_episode:\n",
    "            # Convertir estado a tensor: (1, 4, 84, 84)\n",
    "            state_np = np.array(state, dtype=np.float32).copy()\n",
    "            state_tensor = torch.tensor(state_np / 255.0, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                action_logits, value = model(state_tensor)\n",
    "                probs = torch.softmax(action_logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                action = dist.sample()\n",
    "                log_prob = dist.log_prob(action)\n",
    "            \n",
    "            # Ejecutar acci√≥n (SkipFrame ya repite 4 veces)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # ========== REWARD SHAPING CONSERVADOR ==========\n",
    "            shaped_reward = reward / 15.0  # Normalizar reward del entorno\n",
    "            \n",
    "            # Bonus suave por avanzar a la derecha\n",
    "            current_x_pos = info.get('x_pos', prev_x_pos)\n",
    "            x_progress = (current_x_pos - prev_x_pos) / 40.0  # Normalizado\n",
    "            shaped_reward += x_progress\n",
    "            \n",
    "            # Penalidad por morir\n",
    "            if info.get('life', 2) < 2 or done:\n",
    "                if info.get('life', 2) < 2:\n",
    "                    shaped_reward -= 1.0\n",
    "            \n",
    "            # Tracking\n",
    "            max_x_pos = max(max_x_pos, current_x_pos)\n",
    "            prev_x_pos = current_x_pos\n",
    "            \n",
    "            buffer.push(\n",
    "                state_np / 255.0,  # Normalizado\n",
    "                action.item(),\n",
    "                shaped_reward,\n",
    "                value.item(),\n",
    "                log_prob.item(),\n",
    "                float(done)\n",
    "            )\n",
    "            \n",
    "            episode_reward += reward  # Reward real (sin shaping)\n",
    "            episode_shaped += shaped_reward\n",
    "            state = next_state\n",
    "            global_step += 1\n",
    "            episode_length += 1\n",
    "        \n",
    "        # ========== BOOTSTRAP VALUE SI NO TERMIN√ì ==========\n",
    "        if not done:\n",
    "            state_np = np.array(state, dtype=np.float32).copy()\n",
    "            state_tensor = torch.tensor(state_np / 255.0, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                _, last_value = model(state_tensor)\n",
    "                last_value = last_value.item()\n",
    "        else:\n",
    "            last_value = 0\n",
    "        \n",
    "        # ========== C√ÅLCULO DE RETURNS Y ADVANTAGES ==========\n",
    "        returns, advantages = buffer.compute_returns_and_advantages(last_value, gamma, gae_lambda)\n",
    "        \n",
    "        states_tensor = torch.tensor(np.array(buffer.states), dtype=torch.float32).to(device)\n",
    "        actions_tensor = torch.tensor(buffer.actions, dtype=torch.long).to(device)\n",
    "        old_log_probs = torch.tensor(buffer.log_probs, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Normalizar advantages (reduce varianza del gradiente)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        advantages = advantages.to(device)\n",
    "        returns = returns.to(device)\n",
    "        \n",
    "        # ========== ACTUALIZACI√ìN PPO ==========\n",
    "        dataset_size = len(buffer)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            indices = torch.randperm(dataset_size, device=device)\n",
    "            \n",
    "            for start in range(0, dataset_size, batch_size):\n",
    "                end = min(start + batch_size, dataset_size)\n",
    "                batch_idx = indices[start:end]\n",
    "                \n",
    "                b_states = states_tensor[batch_idx]\n",
    "                b_actions = actions_tensor[batch_idx]\n",
    "                b_old_lp = old_log_probs[batch_idx]\n",
    "                b_advantages = advantages[batch_idx]\n",
    "                b_returns = returns[batch_idx]\n",
    "                \n",
    "                # Forward pass\n",
    "                action_logits, values = model(b_states)\n",
    "                probs = torch.softmax(action_logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                new_log_probs = dist.log_prob(b_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # PPO clipped objective\n",
    "                ratio = torch.exp(new_log_probs - b_old_lp)\n",
    "                surr1 = ratio * b_advantages\n",
    "                surr2 = torch.clamp(ratio, 1.0 - clip_ratio, 1.0 + clip_ratio) * b_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Value loss (clipped tambi√©n, como en la implementaci√≥n original de PPO)\n",
    "                value_loss = ((values.squeeze() - b_returns) ** 2).mean()\n",
    "                \n",
    "                # Loss total: actor + critic - entrop√≠a\n",
    "                total_loss = actor_loss + value_coef * value_loss - entropy_coef * entropy\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "        \n",
    "        # ========== LOGGING ==========\n",
    "        total_rewards.append(episode_reward)\n",
    "        avg_10 = np.mean(total_rewards[-10:]) if len(total_rewards) >= 10 else np.mean(total_rewards)\n",
    "        \n",
    "        if (episode + 1) % 10 == 0 or episode == 0:\n",
    "            print(f\"Ep {episode+1:4d}/{num_episodes} | \"\n",
    "                  f\"Reward: {episode_reward:7.1f} | Avg10: {avg_10:7.1f} | \"\n",
    "                  f\"MaxX: {max_x_pos:5.0f} | Steps: {episode_length:4d} | \"\n",
    "                  f\"Best: {best_reward:7.1f}\")\n",
    "        \n",
    "        # Guardar mejor modelo\n",
    "        if episode_reward > best_reward:\n",
    "            best_reward = episode_reward\n",
    "            save_checkpoint(model, total_rewards, checkpoint_path.replace('.pt', '_best.pt'), device)\n",
    "        \n",
    "        # Checkpoint peri√≥dico\n",
    "        if (episode + 1) % checkpoint_freq == 0:\n",
    "            save_checkpoint(model, total_rewards, checkpoint_path, device)\n",
    "            print(f\"  ‚Üí Checkpoint guardado ({checkpoint_path})\")\n",
    "    \n",
    "    # Guardar al final\n",
    "    save_checkpoint(model, total_rewards, checkpoint_path, device)\n",
    "    return total_rewards\n",
    "\n",
    "\n",
    "def save_checkpoint(model, rewards, path, device=\"cpu\"):\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"rewards\": rewards,\n",
    "        \"device\": str(device),\n",
    "    }, path)\n",
    "\n",
    "    \n",
    "def load_checkpoint(model, path, device=\"cpu\"):\n",
    "    checkpoint = torch.load(path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    rewards = checkpoint.get(\"rewards\", [])\n",
    "    model.to(device)\n",
    "    return model, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd5d84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T23:32:03.084726Z",
     "iopub.status.busy": "2026-02-21T23:32:03.084523Z",
     "iopub.status.idle": "2026-02-21T23:32:03.483320Z",
     "shell.execute_reply": "2026-02-21T23:32:03.482165Z"
    },
    "papermill": {
     "duration": 0.406679,
     "end_time": "2026-02-21T23:32:03.484833",
     "exception": false,
     "start_time": "2026-02-21T23:32:03.078154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cpu\n",
      "Par√°metros totales: 1,687,206\n",
      "Comenzando entrenamiento desde cero. ([Errno 2] No such file or directory: 'mario_ppo.pt')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_737319/816478130.py:178: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=== CONFIGURACI√ìN INICIAL DEL ENTRENAMIENTO ===\n",
    "\n",
    "Inicializamos el dispositivo (GPU si est√° disponible) y el modelo PPO Actor-Critic.\n",
    "Si existe un checkpoint anterior, lo cargamos para continuar el entrenamiento.\n",
    "\"\"\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Dispositivo: {device}\")\n",
    "\n",
    "# Inicializar modelo\n",
    "model = PPOActorCritic(input_shape, num_actions, hidden_size=512)\n",
    "print(f\"Par√°metros totales: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Intentar cargar checkpoint anterior\n",
    "checkpoint_path = \"mario_ppo.pt\"\n",
    "try:\n",
    "    model, prev_rewards = load_checkpoint(model, checkpoint_path, device)\n",
    "    print(f\"Reanudando: {len(prev_rewards)} episodios previos, mejor reward: {max(prev_rewards):.1f}\")\n",
    "except (FileNotFoundError, RuntimeError) as e:\n",
    "    print(f\"Comenzando entrenamiento desde cero. ({e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0fdf27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T23:32:03.497873Z",
     "iopub.status.busy": "2026-02-21T23:32:03.497638Z",
     "iopub.status.idle": "2026-02-21T23:47:16.839453Z",
     "shell.execute_reply": "2026-02-21T23:47:16.837512Z"
    },
    "papermill": {
     "duration": 913.350667,
     "end_time": "2026-02-21T23:47:16.841763",
     "exception": false,
     "start_time": "2026-02-21T23:32:03.491096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%skip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear entorno con wrappers est√°ndar\n",
    "env = make_mario_env(world=1, stage=1)\n",
    "\n",
    "# Verificar\n",
    "test_state, _ = env.reset()\n",
    "print(f\"Estado shape: {test_state.shape} (frames, height, width)\")\n",
    "print(f\"Action space: {env.action_space.n} acciones\")\n",
    "print(f\"Dtype: {np.array(test_state).dtype}, Rango: [{np.array(test_state).min()}, {np.array(test_state).max()}]\")\n",
    "\n",
    "# ========== ENTRENAMIENTO ==========\n",
    "num_episodes = 500\n",
    "total_rewards = train_ppo(\n",
    "    env,\n",
    "    model,\n",
    "    num_episodes=num_episodes,\n",
    "    num_epochs=4,\n",
    "    batch_size=128,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    learning_rate=2.5e-4,\n",
    "    clip_ratio=0.2,\n",
    "    entropy_coef=0.01,\n",
    "    value_coef=0.5,\n",
    "    device=device,\n",
    "    checkpoint_freq=25,\n",
    "    checkpoint_path=\"mario_ppo.pt\",\n",
    "    max_steps_per_episode=2000,\n",
    ")\n",
    "\n",
    "# ========== VISUALIZACI√ìN ==========\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(total_rewards, alpha=0.4, label='Episode Reward', color='steelblue')\n",
    "if len(total_rewards) >= 10:\n",
    "    avg = np.convolve(total_rewards, np.ones(10)/10, mode='valid')\n",
    "    plt.plot(range(9, len(total_rewards)), avg, label='Media m√≥vil (10)', linewidth=2, color='red')\n",
    "plt.xlabel('Episodio')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Recompensas por Episodio')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "window = min(100, len(total_rewards))\n",
    "recent = total_rewards[-window:]\n",
    "plt.plot(range(len(total_rewards) - window, len(total_rewards)), recent, alpha=0.4, color='steelblue')\n",
    "if len(recent) >= 10:\n",
    "    avg_r = np.convolve(recent, np.ones(10)/10, mode='valid')\n",
    "    plt.plot(range(len(total_rewards) - window + 9, len(total_rewards)), avg_r,\n",
    "             linewidth=2, color='red', label='Media m√≥vil (10)')\n",
    "plt.xlabel('Episodio')\n",
    "plt.ylabel('Reward')\n",
    "plt.title(f'√öltimos {window} Episodios')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== ESTAD√çSTICAS ==========\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ESTAD√çSTICAS FINALES\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Reward promedio (√∫ltimos 10): {np.mean(total_rewards[-10:]):.1f}\")\n",
    "if len(total_rewards) >= 50:\n",
    "    print(f\"Reward promedio (√∫ltimos 50): {np.mean(total_rewards[-50:]):.1f}\")\n",
    "print(f\"Reward m√°ximo: {max(total_rewards):.1f}\")\n",
    "print(f\"Reward m√≠nimo: {min(total_rewards):.1f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "diagnostico_training(total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c713c9",
   "metadata": {
    "papermill": {
     "duration": 0.012133,
     "end_time": "2026-02-21T23:47:16.866920",
     "exception": false,
     "start_time": "2026-02-21T23:47:16.854787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inicio del Entrenamiento\n",
    "\n",
    "A continuaci√≥n entrenaremos el modelo PPO con la configuraci√≥n est√°ndar:\n",
    "\n",
    "**Arquitectura:**\n",
    "- CNN Actor-Critic (Nature DQN backbone + Actor/Critic heads)\n",
    "- Input: 4 frames en escala de grises de 84√ó84\n",
    "- Inicializaci√≥n ortogonal para mejor estabilidad\n",
    "\n",
    "**Par√°metros:**\n",
    "- **num_episodes=500**: Episodios de entrenamiento\n",
    "- **num_epochs=4**: Reutilizar datos 4 veces por episodio\n",
    "- **batch_size=128**: Tama√±o de mini-batch\n",
    "- **learning_rate=2.5e-4**: Tasa de aprendizaje est√°ndar para PPO\n",
    "- **clip_ratio=0.2**: PPO clipping\n",
    "- **entropy_coef=0.01**: Incentivo de exploraci√≥n\n",
    "- **gamma=0.99, lambda=0.95**: Descuento y GAE\n",
    "\n",
    "**Expectativas:**\n",
    "- Primeros ~50 episodios: exploraci√≥n, rewards bajos\n",
    "- Episodios 50-200: aprende a avanzar hacia la derecha\n",
    "- Episodios 200+: deber√≠a mejorar progresivamente, pasando obst√°culos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18fb2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T23:47:16.893063Z",
     "iopub.status.busy": "2026-02-21T23:47:16.892719Z",
     "iopub.status.idle": "2026-02-21T23:47:19.457031Z",
     "shell.execute_reply": "2026-02-21T23:47:19.456300Z"
    },
    "papermill": {
     "duration": 2.57887,
     "end_time": "2026-02-21T23:47:19.458349",
     "exception": false,
     "start_time": "2026-02-21T23:47:16.879479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando modelo actual (no se encontr√≥ best checkpoint)\n",
      "Evaluando modelo entrenado (5 episodios)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_737319/816478130.py:178: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n",
      "/home/julian/.pyenv/versions/Monografia/lib/python3.8/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3` with the environment ID `SuperMarioBros-1-1-v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Episodio 1: Reward =  -400.0 | Steps = 2000 | Max X = 40\n",
      "  Episodio 2: Reward =  -400.0 | Steps = 2000 | Max X = 40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m test_x_positions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m---> 51\u001b[0m     reward, steps, max_x \u001b[38;5;241m=\u001b[39m \u001b[43mplay_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     test_rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     53\u001b[0m     test_x_positions\u001b[38;5;241m.\u001b[39mappend(max_x)\n",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m, in \u001b[0;36mplay_episode\u001b[0;34m(env, model, device, max_steps)\u001b[0m\n\u001b[1;32m     21\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state_np \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 24\u001b[0m     action_logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     action \u001b[38;5;241m=\u001b[39m action_logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.pyenv/versions/Monografia/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/Monografia/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m, in \u001b[0;36mPPOActorCritic.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    x: (batch_size, channels, height, width) ‚Äî channels = 4 frames apilados\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared_fc(features)\n\u001b[1;32m     61\u001b[0m     action_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(features)\n",
      "File \u001b[0;32m~/.pyenv/versions/Monografia/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/Monografia/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/Monografia/lib/python3.8/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/Monografia/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/Monografia/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/Monografia/lib/python3.8/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/Monografia/lib/python3.8/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%skip\n",
    "\"\"\"\n",
    "=== INFERENCIA: USAR EL MODELO ENTRENADO ===\n",
    "\n",
    "Jugamos episodios usando la pol√≠tica aprendida con acci√≥n determinista (greedy).\n",
    "\"\"\"\n",
    "\n",
    "def play_episode(env, model, device=\"cpu\", max_steps=2000):\n",
    "    \"\"\"\n",
    "    Juega un episodio completo usando la pol√≠tica aprendida.\n",
    "    Usa acci√≥n greedy (argmax) en vez de sampling para evaluar.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    state, _info = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    max_x = 40\n",
    "    \n",
    "    while not done and steps < max_steps:\n",
    "        state_np = np.array(state, dtype=np.float32).copy()\n",
    "        state_tensor = torch.tensor(state_np / 255.0, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_logits, _ = model(state_tensor)\n",
    "            action = action_logits.argmax(dim=-1)\n",
    "        \n",
    "        state, reward, terminated, truncated, info = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        max_x = max(max_x, info.get('x_pos', 0))\n",
    "        steps += 1\n",
    "    \n",
    "    model.train()\n",
    "    return total_reward, steps, max_x\n",
    "\n",
    "# ===== EVALUACI√ìN =====\n",
    "# Cargar mejor modelo si existe\n",
    "best_path = \"mario_ppo_best.pt\"\n",
    "try:\n",
    "    model, _ = load_checkpoint(model, best_path, device)\n",
    "    print(f\"Cargado mejor modelo desde {best_path}\")\n",
    "except (FileNotFoundError, RuntimeError):\n",
    "    print(\"Usando modelo actual (no se encontr√≥ best checkpoint)\")\n",
    "\n",
    "print(\"Evaluando modelo entrenado (5 episodios)...\\n\")\n",
    "\n",
    "eval_env = make_mario_env(world=1, stage=1)\n",
    "test_rewards = []\n",
    "test_x_positions = []\n",
    "\n",
    "for i in range(5):\n",
    "    reward, steps, max_x = play_episode(eval_env, model, device=device)\n",
    "    test_rewards.append(reward)\n",
    "    test_x_positions.append(max_x)\n",
    "    print(f\"  Episodio {i+1}: Reward = {reward:7.1f} | Steps = {steps:4d} | Max X = {max_x:.0f}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Reward promedio:  {np.mean(test_rewards):.1f} ¬± {np.std(test_rewards):.1f}\")\n",
    "print(f\"Reward m√°ximo:    {max(test_rewards):.1f}\")\n",
    "print(f\"X position avg:   {np.mean(test_x_positions):.0f}\")\n",
    "print(f\"X position max:   {max(test_x_positions):.0f}\")\n",
    "print(f\"{'='*50}\")\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f2b42",
   "metadata": {},
   "source": [
    "## Entrenamiento con Stable Baselines3 (Entornos Paralelos)\n",
    "\n",
    "La implementaci√≥n manual anterior es √∫til para entender el algoritmo PPO en detalle, pero tiene una limitaci√≥n fundamental: **usa un solo entorno**. El PPO est√°ndar est√° dise√±ado para recolectar experiencias de **m√∫ltiples entornos en paralelo**, lo que:\n",
    "\n",
    "1. **Reduce la varianza** de los gradientes: en vez de actualizar con datos de 1 episodio ruidoso, se promedian experiencias de N entornos simult√°neos.\n",
    "2. **Genera rollouts de tama√±o fijo** (ej. 512 steps √ó 8 envs = 4096 steps por update) en lugar de rollouts de longitud variable por episodio.\n",
    "3. **Mejora la eficiencia muestral**: m√°s datos diversos por actualizaci√≥n.\n",
    "\n",
    "Usaremos **Stable Baselines3** (SB3), la librer√≠a de referencia para RL, que implementa PPO con todas estas optimizaciones. Internamente, SB3 usa la misma arquitectura `CnnPolicy` (Nature DQN backbone) que implementamos manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba25459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3 in ./.venv/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in ./.venv/lib/python3.12/site-packages (from stable-baselines3) (1.2.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in ./.venv/lib/python3.12/site-packages (from stable-baselines3) (1.26.4)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in ./.venv/lib/python3.12/site-packages (from stable-baselines3) (2.10.0)\n",
      "Requirement already satisfied: cloudpickle in ./.venv/lib/python3.12/site-packages (from stable-baselines3) (3.1.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from stable-baselines3) (3.0.0)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (from stable-baselines3) (3.10.8)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./.venv/lib/python3.12/site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./.venv/lib/python3.12/site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.20.3)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (82.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (2026.2.0)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in ./.venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in ./.venv/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch<3.0,>=2.3->stable-baselines3) (1.3.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./.venv/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae2510b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym_super_mario_bros' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _init\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# DummyVecEnv ejecuta los entornos secuencialmente en el mismo proceso.\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# SubprocVecEnv (multiprocessing) no es compatible con gym_super_mario_bros\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# porque el emulador NES no soporta pickle/fork correctamente.\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# DummyVecEnv es m√°s lento pero funciona de forma confiable.\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# VecMonitor registra m√©tricas autom√°ticamente (episode reward, length, etc.)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m vec_env = VecMonitor(\u001b[43mDummyVecEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmake_env_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_envs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_envs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m entornos paralelos creados\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Observation space: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvec_env.observation_space.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/sdb2/Documents/Redes-Neuronales/monograf√≠a/.venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:31\u001b[39m, in \u001b[36mDummyVecEnv.__init__\u001b[39m\u001b[34m(self, env_fns)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns: \u001b[38;5;28mlist\u001b[39m[Callable[[], gym.Env]]):\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28mself\u001b[39m.envs = [_patch_env(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mid\u001b[39m(env.unwrapped) \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.envs])) != \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.envs):\n\u001b[32m     33\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     34\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33minstead of creating different objects. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m         )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mmake_env_fn.<locals>._init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_init\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_mario_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworld\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mmake_mario_env\u001b[39m\u001b[34m(world, stage)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_mario_env\u001b[39m(world=\u001b[32m1\u001b[39m, stage=\u001b[32m1\u001b[39m):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Crea el entorno de Mario con todos los wrappers est√°ndar de RL.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     env = \u001b[43mgym_super_mario_bros\u001b[49m.make(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mSuperMarioBros-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworld\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-v0\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     65\u001b[39m     env = JoypadSpace(env, RIGHT_ONLY)\n\u001b[32m     66\u001b[39m     env = CompatResetWrapper(env)  \u001b[38;5;66;03m# Absorbe seed/options para SB3\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'gym_super_mario_bros' is not defined"
     ]
    }
   ],
   "source": [
    "# ========== CALLBACK PARA LOGGING ==========\n",
    "class MarioCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback personalizado para registrar m√©tricas durante el entrenamiento.\n",
    "    SB3 maneja el training loop internamente, as√≠ que usamos callbacks\n",
    "    para monitorear progreso y guardar checkpoints.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq=10000, save_path=\"mario_sb3.pt\", verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        self.episode_rewards = []\n",
    "        self.episode_x_positions = []\n",
    "        self.best_mean_reward = -float('inf')\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Recolectar m√©tricas de episodios completados\n",
    "        if self.locals.get(\"infos\"):\n",
    "            for info in self.locals[\"infos\"]:\n",
    "                if \"episode\" in info:\n",
    "                    self.episode_rewards.append(info[\"episode\"][\"r\"])\n",
    "                if \"x_pos\" in info:\n",
    "                    self.episode_x_positions.append(info[\"x_pos\"])\n",
    "\n",
    "        # Checkpoint peri√≥dico\n",
    "        if self.n_calls % self.check_freq == 0 and len(self.episode_rewards) >= 10:\n",
    "            mean_reward = np.mean(self.episode_rewards[-10:])\n",
    "            if self.verbose:\n",
    "                print(f\"  Step {self.n_calls:>7d} | \"\n",
    "                      f\"Episodes: {len(self.episode_rewards):>4d} | \"\n",
    "                      f\"Avg10 Reward: {mean_reward:>7.1f} | \"\n",
    "                      f\"Best Avg10: {self.best_mean_reward:>7.1f}\")\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "                self.best_mean_reward = mean_reward\n",
    "                self.model.save(self.save_path.replace(\".pt\", \"_best\"))\n",
    "                if self.verbose:\n",
    "                    print(f\"  ‚Üí Nuevo mejor modelo guardado (avg10={mean_reward:.1f})\")\n",
    "        return True\n",
    "\n",
    "# ========== CREAR ENTORNOS VECTORIZADOS ==========\n",
    "n_envs = 8  # 8 entornos simult√°neos\n",
    "\n",
    "def make_env_fn(world=1, stage=1):\n",
    "    \"\"\"Factory function para DummyVecEnv: retorna una funci√≥n creadora de env.\"\"\"\n",
    "    def _init():\n",
    "        return make_mario_env(world=world, stage=stage)\n",
    "    return _init\n",
    "\n",
    "# DummyVecEnv ejecuta los entornos secuencialmente en el mismo proceso.\n",
    "# SubprocVecEnv (multiprocessing) no es compatible con gym_super_mario_bros\n",
    "# porque el emulador NES no soporta pickle/fork correctamente.\n",
    "# DummyVecEnv es m√°s lento pero funciona de forma confiable.\n",
    "# VecMonitor registra m√©tricas autom√°ticamente (episode reward, length, etc.)\n",
    "vec_env = VecMonitor(DummyVecEnv([make_env_fn() for _ in range(n_envs)]))\n",
    "\n",
    "print(f\"‚úÖ {n_envs} entornos paralelos creados\")\n",
    "print(f\"   Observation space: {vec_env.observation_space.shape}\")\n",
    "print(f\"   Action space: {vec_env.action_space.n} acciones\")\n",
    "\n",
    "# ========== CONFIGURAR Y ENTRENAR PPO ==========\n",
    "sb3_callback = MarioCallback(check_freq=10000, save_path=\"mario_sb3.pt\")\n",
    "\n",
    "sb3_model = PPO(\n",
    "    \"CnnPolicy\",                # Misma arquitectura Nature DQN que usamos manualmente\n",
    "    vec_env,\n",
    "    n_steps=512,                # Steps por env antes de cada update (512 √ó 8 = 4096 total)\n",
    "    batch_size=256,             # Mini-batch size para las √©pocas de actualizaci√≥n\n",
    "    n_epochs=4,                 # 4 pasadas sobre los datos por update\n",
    "    learning_rate=2.5e-4,       # Mismo LR que nuestra implementaci√≥n\n",
    "    clip_range=0.2,             # Mismo clip ratio\n",
    "    ent_coef=0.01,              # Mismo coeficiente de entrop√≠a\n",
    "    vf_coef=0.5,                # Mismo peso del value loss\n",
    "    gamma=0.99,                 # Descuento\n",
    "    gae_lambda=0.95,            # GAE lambda\n",
    "    max_grad_norm=0.5,          # Gradient clipping\n",
    "    verbose=0,                  # Silenciar output por defecto (usamos callback)\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(f\"\\nüéÆ Iniciando entrenamiento SB3 PPO ({n_envs} envs paralelos)\")\n",
    "print(f\"   Total timesteps: 2,000,000\")\n",
    "print(f\"   Steps por update: {512 * n_envs} ({512} √ó {n_envs} envs)\")\n",
    "print(f\"   Device: {device}\\n\")\n",
    "\n",
    "sb3_model.learn(\n",
    "    total_timesteps=2_000_000,\n",
    "    callback=sb3_callback,\n",
    "    progress_bar=True,\n",
    ")\n",
    "\n",
    "# Guardar modelo final\n",
    "sb3_model.save(\"mario_sb3_final\")\n",
    "vec_env.close()\n",
    "print(f\"\\n‚úÖ Entrenamiento SB3 completado. Modelo guardado en mario_sb3_final.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92b894",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ========== EVALUACI√ìN Y COMPARACI√ìN SB3 ==========\n",
    "\n",
    "# Cargar el mejor modelo SB3\n",
    "best_sb3_path = \"mario_sb3_best\"\n",
    "if os.path.exists(best_sb3_path + \".zip\"):\n",
    "    sb3_eval_model = PPO.load(best_sb3_path, device=device)\n",
    "    print(f\"Cargado mejor modelo SB3 desde {best_sb3_path}.zip\")\n",
    "else:\n",
    "    sb3_eval_model = PPO.load(\"mario_sb3_final\", device=device)\n",
    "    print(\"Usando modelo final SB3\")\n",
    "\n",
    "# Evaluar en 10 episodios\n",
    "eval_env = make_mario_env(world=1, stage=1)\n",
    "sb3_rewards = []\n",
    "sb3_positions = []\n",
    "\n",
    "print(\"\\nEvaluando modelo SB3 (10 episodios, greedy)...\\n\")\n",
    "for i in range(10):\n",
    "    obs, info = eval_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    max_x = 40\n",
    "    steps = 0\n",
    "\n",
    "    while not done and steps < 4000:\n",
    "        # SB3 usa deterministic=True para evaluaci√≥n greedy\n",
    "        action, _ = sb3_eval_model.predict(np.array(obs), deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        max_x = max(max_x, info.get('x_pos', 0))\n",
    "        steps += 1\n",
    "\n",
    "    sb3_rewards.append(total_reward)\n",
    "    sb3_positions.append(max_x)\n",
    "    print(f\"  Ep {i+1:2d}: Reward = {total_reward:7.1f} | Steps = {steps:4d} | Max X = {max_x:.0f}\")\n",
    "\n",
    "eval_env.close()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RESULTADOS SB3 ({n_envs} envs paralelos, 2M steps)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Reward promedio:  {np.mean(sb3_rewards):.1f} ¬± {np.std(sb3_rewards):.1f}\")\n",
    "print(f\"Reward m√°ximo:    {max(sb3_rewards):.1f}\")\n",
    "print(f\"X position avg:   {np.mean(sb3_positions):.0f}\")\n",
    "print(f\"X position max:   {max(sb3_positions):.0f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# ========== GR√ÅFICO COMPARATIVO ==========\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Curva de entrenamiento SB3\n",
    "if sb3_callback.episode_rewards:\n",
    "    rewards_sb3 = sb3_callback.episode_rewards\n",
    "    axes[0].plot(rewards_sb3, alpha=0.3, color='steelblue', label='Episode Reward')\n",
    "    if len(rewards_sb3) >= 50:\n",
    "        avg = np.convolve(rewards_sb3, np.ones(50)/50, mode='valid')\n",
    "        axes[0].plot(range(49, len(rewards_sb3)), avg,\n",
    "                     label='Media m√≥vil (50)', linewidth=2, color='red')\n",
    "    axes[0].set_xlabel('Episodio')\n",
    "    axes[0].set_ylabel('Reward')\n",
    "    axes[0].set_title('SB3 PPO - Curva de Entrenamiento')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Comparaci√≥n de evaluaci√≥n\n",
    "labels = ['PPO Manual\\n(1 env, 500 eps)', 'SB3 PPO\\n(8 envs, 2M steps)']\n",
    "means = [np.mean(test_rewards), np.mean(sb3_rewards)]\n",
    "stds = [np.std(test_rewards), np.std(sb3_rewards)]\n",
    "colors = ['#ff7f0e', '#2ca02c']\n",
    "\n",
    "bars = axes[1].bar(labels, means, yerr=stds, capsize=5, color=colors, alpha=0.8, edgecolor='black')\n",
    "axes[1].set_ylabel('Reward Promedio (eval)')\n",
    "axes[1].set_title('Comparaci√≥n: PPO Manual vs SB3')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Anotar valores\n",
    "for bar, mean, std in zip(bars, means, stds):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 20,\n",
    "                 f'{mean:.0f} ¬± {std:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4650cdbc",
   "metadata": {},
   "source": [
    "## Comparaci√≥n y Conclusiones\n",
    "\n",
    "| Aspecto | PPO Manual (1 env) | SB3 PPO (8 envs) |\n",
    "|---------|-------------------|-------------------|\n",
    "| **Entornos** | 1 secuencial | 8 paralelos (DummyVecEnv) |\n",
    "| **Rollout** | Por episodio (variable) | Fijo: 512 steps √ó 8 envs = 4096 |\n",
    "| **Varianza** | Alta (1 trayectoria por update) | Baja (8 trayectorias promediadas) |\n",
    "| **Arquitectura** | PPOActorCritic (manual) | CnnPolicy (Nature DQN, equivalente) |\n",
    "| **Hiperpar√°metros** | Id√©nticos (LR, clip, entropy, etc.) | Id√©nticos |\n",
    "\n",
    "### ¬øPor qu√© SB3 deber√≠a rendir mejor?\n",
    "\n",
    "La diferencia clave no est√° en la arquitectura ni en los hiperpar√°metros (son los mismos), sino en **c√≥mo se recolectan las experiencias**:\n",
    "\n",
    "- **PPO Manual**: Cada update usa datos de ~100-300 steps de un solo episodio. Si Mario muri√≥ r√°pido en ese episodio, el gradiente ser√° muy ruidoso.\n",
    "- **SB3**: Cada update usa 4096 steps recolectados de 8 entornos independientes. Esto promedia experiencias buenas y malas, dando gradientes m√°s estables y representativos.\n",
    "\n",
    "Esto es la misma raz√≥n por la cual en deep learning se usan mini-batches grandes en vez de SGD puro: **reducir la varianza del gradiente permite dar pasos m√°s confiables hacia el √≥ptimo**."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "3.10.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 940.088738,
   "end_time": "2026-02-21T23:47:21.826270",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-21T23:31:41.737532",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
