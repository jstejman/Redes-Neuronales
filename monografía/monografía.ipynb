{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monografía Final\n",
    "## Redes Neuronales - FIUBA\n",
    "### Alumno: Julián Stejman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este documento se utilizará una red neuronal entrenada con un algoritmo de aprendizaje por refuerzo que aprendiese a jugar niveles de Super Mario Bros para la NES. Para ello, se utilizará el entorno de OpenAI Gym, que simula el juego de Super Mario Bros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T02:52:48.222057Z",
     "iopub.status.busy": "2025-01-28T02:52:48.221769Z",
     "iopub.status.idle": "2025-01-28T02:52:51.477368Z",
     "shell.execute_reply": "2025-01-28T02:52:51.476500Z",
     "shell.execute_reply.started": "2025-01-28T02:52:48.222036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym_super_mario_bros in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (7.4.0)\n",
      "Requirement already satisfied: nes-py>=8.1.4 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from gym_super_mario_bros) (8.2.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (1.24.4)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (1.5.21)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (4.67.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym_super_mario_bros) (3.1.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym_super_mario_bros) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym_super_mario_bros) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from importlib-metadata>=4.10.0->gym>=0.17.2->nes-py>=8.1.4->gym_super_mario_bros) (3.20.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T02:52:51.479001Z",
     "iopub.status.busy": "2025-01-28T02:52:51.478760Z",
     "iopub.status.idle": "2025-01-28T02:52:51.483215Z",
     "shell.execute_reply": "2025-01-28T02:52:51.482386Z",
     "shell.execute_reply.started": "2025-01-28T02:52:51.478980Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se eliminara en la siguiente celda la línea que dice %%skip, se podría ver una simulación simple de como funciona el emulador del juego con el entorno. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T02:52:51.484931Z",
     "iopub.status.busy": "2025-01-28T02:52:51.484722Z",
     "iopub.status.idle": "2025-01-28T02:52:51.497341Z",
     "shell.execute_reply": "2025-01-28T02:52:51.496615Z",
     "shell.execute_reply.started": "2025-01-28T02:52:51.484914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%%skip\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "done = True\n",
    "for step in range(5000):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea sería armar una red neuronal convolucional para poder interpretar de las imágenes recibidas una posible acción para que tomara el personaje del juego. Esta red luego utilizará un algoritmo de aprendizaje por refuerzo para poder mejorar su desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En rigor, la salida no es una acción directamente sino lo que se llaman Q-valores, que le generan un puntaje de calidad a cada acción a tomar. Mientras más alto el puntaje más viable sería la acción en el momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T02:52:51.499110Z",
     "iopub.status.busy": "2025-01-28T02:52:51.498858Z",
     "iopub.status.idle": "2025-01-28T02:52:51.512172Z",
     "shell.execute_reply": "2025-01-28T02:52:51.511526Z",
     "shell.execute_reply.started": "2025-01-28T02:52:51.499089Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self,input_shape,num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        #Capas convolucionales\n",
    "        self.image_processing = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride =1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Capas lineales para predicción de Q-valores\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self._image_processing_output_size(input_shape),512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,num_actions)\n",
    "        )\n",
    "        # función que genera el tamaño de salida para la etapa lineal \n",
    "    def _image_processing_output_size(self, input_shape):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)\n",
    "            output=self.image_processing(dummy_input)\n",
    "            return int(torch.flatten(output, start_dim = 1).size(1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.image_processing(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T02:52:51.513248Z",
     "iopub.status.busy": "2025-01-28T02:52:51.513005Z",
     "iopub.status.idle": "2025-01-28T02:52:51.716752Z",
     "shell.execute_reply": "2025-01-28T02:52:51.716046Z",
     "shell.execute_reply.started": "2025-01-28T02:52:51.513229Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0360, -0.0259, -0.0259, -0.0205,  0.0254, -0.0104,  0.0029]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "frame_stack_count = 4\n",
    "dimensions = 84\n",
    "input_shape = (frame_stack_count,dimensions,dimensions)\n",
    "num_actions = len(SIMPLE_MOVEMENT)\n",
    "\n",
    "test_model = DQNetwork(input_shape, num_actions)\n",
    "dummy_state = torch.zeros(1, *input_shape)\n",
    "output = test_model(dummy_state)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para enviar a la red, se toma la decisión de mandar los colores a blanco y negro,bajarle la resolución a 84x84 que es la que se suele utilizar para este tipo de ensayos, y de devolver en cada estado 4 cuadros. De esta forma la red podrá aprender en cada estado con más información del entorno y menos en definición visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T02:52:51.717693Z",
     "iopub.status.busy": "2025-01-28T02:52:51.717443Z",
     "iopub.status.idle": "2025-01-28T02:52:52.335396Z",
     "shell.execute_reply": "2025-01-28T02:52:52.334565Z",
     "shell.execute_reply.started": "2025-01-28T02:52:51.717663Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/.pyenv/versions/Monografia/lib/python3.8/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3` with the environment ID `SuperMarioBros-v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from gym.wrappers import GrayScaleObservation, FrameStack, ResizeObservation\n",
    "\n",
    "# Create the Super Mario environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "\n",
    "# Apply the wrappers for preprocessing\n",
    "env = GrayScaleObservation(env, keep_dim=True)  # Convert to grayscale (removes color channels)\n",
    "env = ResizeObservation(env, shape=(dimensions,dimensions))     # Resize to 84x84\n",
    "env = FrameStack(env, num_stack=frame_stack_count)               # Stack 4 frames\n",
    "\n",
    "# Reset the environment to start\n",
    "state = env.reset()\n",
    "\n",
    "# Visualize the preprocessed frame (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(state[0], cmap='gray')  # Show the first frame in the stack\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder guardar los resultados de cada estado, se utiliza un replay buffer que simplemente guarda los resultados para ver cómo repercuten las acciones que toma la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T02:52:52.336528Z",
     "iopub.status.busy": "2025-01-28T02:52:52.336210Z",
     "iopub.status.idle": "2025-01-28T02:52:52.341433Z",
     "shell.execute_reply": "2025-01-28T02:52:52.340560Z",
     "shell.execute_reply.started": "2025-01-28T02:52:52.336497Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self,capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self,state,action,reward,next_state,done):\n",
    "        self.buffer.append((state,action,reward,next_state,done))\n",
    "    def sample(self,batch_size):\n",
    "        return random.sample(self.buffer,batch_size)\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar a la red hay que tomar varias consideraciones. \n",
    "Inicialmente se propone un criterio de decisión que se llama epsilon-greedy que lo que hace es ir disminuyendo las opciones elegidas al azar al paso del tiempo. El enfoque de esto es para proponer una solución al asunto de \"explorar vs explotar\" donde un jugador podría maximizar su \"premio\" y no necesariamente buscar un camino más fructífero pero a la vez explorar mucho podría llegar a no maximizar nada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo DQN se basa principalmente en maximizar la función de Bellman que tiene la siguiente forma\n",
    "$$Q(s,a) = r + \\gamma \\cdot max_{a'} Q(s', a') $$\n",
    "Donde r son los premios, $\\gamma$ un factor escalar y los pares (s,a) y (s',a') siendo el par de estado-acción actual y futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T02:52:52.343803Z",
     "iopub.status.busy": "2025-01-28T02:52:52.343596Z",
     "iopub.status.idle": "2025-01-28T02:52:52.357131Z",
     "shell.execute_reply": "2025-01-28T02:52:52.356442Z",
     "shell.execute_reply.started": "2025-01-28T02:52:52.343785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import numpy as np\n",
    "def trainDQN(env, policy_network, target_network, replay_buffer, num_episodes=10,  # Reduce number of episodes for testing\n",
    "             batch_size=64, gamma=0.99, learning_rate=1e-4, target_update_freq=1000, \n",
    "             epsilon_start=1.0, epsilon_decay=0.999, epsilon_min=0.01,\n",
    "             num_stack=4, width=84, height=84, render=False, device=\"cpu\"):\n",
    "    policy_network.to(device)\n",
    "    target_network.to(device)\n",
    "    optimizer = optim.Adam(policy_network.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "    steps = 0\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        print(f\"Starting episode {episode}\")\n",
    "        state = env.reset()\n",
    "        state = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0).to(device)  # Se agrega una dimensión de batch\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Estrategia ε-greedy \n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()  # Explore\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = policy_network(state.squeeze(-1))\n",
    "                    action = torch.argmax(q_values).item()  # Exploit\n",
    "\n",
    "            # Execute action in environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Store transition in replay buffer\n",
    "            replay_buffer.push(state.cpu(), action, reward, next_state.cpu(), done)\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "            # Print replay buffer size\n",
    "            #print(f\"Replay buffer size: {replay_buffer.size()}\")\n",
    "            \n",
    "\n",
    "            # Print info to understand why the episode ended\n",
    "            if done:\n",
    "                print(f\"Episode ended: {info}\")\n",
    "                break  # Break out of the loop to start a new episode\n",
    "\n",
    "            # Train the policy network if we have enough samples\n",
    "            if replay_buffer.size() >= batch_size:\n",
    "                #print(\"Training the policy network\")\n",
    "                # Sample a batch of transitions\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "                # Convert to tensors\n",
    "                states = torch.cat(states).view(batch_size, num_stack, width, height).to(device)\n",
    "                actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1).to(device)\n",
    "                rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "                next_states = torch.cat(next_states).view(batch_size, num_stack, width, height).to(device)\n",
    "                dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "                # Compute current Q-values\n",
    "                q_values = policy_network(states).gather(1, actions)\n",
    "\n",
    "                # Compute target Q-values\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = target_network(next_states).max(1, keepdim=True)[0]\n",
    "                    target_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(q_values, target_q_values)\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Update target network periodically\n",
    "            steps += 1\n",
    "            if steps % target_update_freq == 0:\n",
    "                print(f\"Time: {info['time']}, Lives: {info['life']}\")\n",
    "                print(\"Updating target network\")\n",
    "                target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        # Log episode results\n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode} finished, Reward: {episode_reward}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    env.close()\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el algoritmo de entrenamiento DQN se utilizan 2 modelos en simultáneo, una red llamada la red de políticas, que es la que activamente está aprendiendo y se utiliza para seleccionar acciones durante el juego, y la red objetivo, que recibe menos actualizaciones pero es más estable. Esta última además se utiliza para el cálculo de Q-valores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Guardar el estado local para continuar el entrenamiento\n",
    "def save_training_state(filename, policy_network, target_network, buffer, total_rewards):\n",
    "    state = {\n",
    "        'policy_network': policy_network.state_dict(),\n",
    "        'target_network': target_network.state_dict(),\n",
    "        'replay_buffer': buffer.buffer,\n",
    "        'total_rewards': total_rewards\n",
    "    }\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(state, f)\n",
    "\n",
    "def load_training_state(filename, policy_network, target_network, buffer):\n",
    "    with open(filename, 'rb') as f:\n",
    "        state = pickle.load(f)\n",
    "    policy_network.load_state_dict(state['policy_network'])\n",
    "    target_network.load_state_dict(state['target_network'])\n",
    "    buffer.buffer = state['replay_buffer']\n",
    "    return state['total_rewards']\n",
    "    \n",
    "\n",
    "# Ejemplo de uso:\n",
    "# save_training_state('mario_dqn_state.pkl', policy_network, target_network, buffer, total_rewards)\n",
    "# total_rewards = load_training_state('mario_dqn_state.pkl', policy_network, target_network, buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T02:52:52.358439Z",
     "iopub.status.busy": "2025-01-28T02:52:52.358089Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Starting episode 1\n",
      "Time: 358, Lives: 1\n",
      "Updating target network\n",
      "Time: 308, Lives: 1\n",
      "Updating target network\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "import os\n",
    "\n",
    "# Se genera el entorno de juego \n",
    "width = 84\n",
    "height = 84\n",
    "num_stack = 4\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n",
    "env = GrayScaleObservation(env, keep_dim=True) \n",
    "env = ResizeObservation(env, shape=(width,height))     \n",
    "env = FrameStack(env, num_stack)   \n",
    "\n",
    "# Inicializo redes\n",
    "input_shape = (num_stack, width, height)\n",
    "num_actions = env.action_space.n\n",
    "policy_network = DQNetwork(input_shape, num_actions)\n",
    "target_network = DQNetwork(input_shape, num_actions)\n",
    "target_network.load_state_dict(policy_network.state_dict())\n",
    "target_network.eval()\n",
    "\n",
    "# Creo el buffer\n",
    "buffer = ReplayBuffer(10000)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if os.path.exists('mario_dqn_state.pkl'):\n",
    "    total_rewards = load_training_state('mario_dqn_state.pkl', policy_network, target_network, buffer)\n",
    "else:\n",
    "    total_rewards = []\n",
    "more_rewards = trainDQN(\n",
    "        env=env,\n",
    "        policy_network=policy_network,\n",
    "        target_network=target_network,\n",
    "        replay_buffer=buffer,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        num_stack=num_stack,\n",
    "        num_episodes=1,  # Number of additional episodes to train\n",
    "        render=False,\n",
    "        device=device\n",
    "    )\n",
    "save_training_state('mario_dqn_state.pkl', policy_network, target_network, buffer, total_rewards + more_rewards)\n",
    "# Ejemplo de uso:\n",
    "# total_rewards = load_training_state('mario_dqn_state.pkl', policy_network, target_network, buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%skip \n",
    "state = torch.tensor(env.reset(), dtype=torch.float32).squeeze(-1).unsqueeze(0)\n",
    "state.shape\n",
    "policy_network(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Monografia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
