{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6f1dbc",
   "metadata": {
    "papermill": {
     "duration": 0.004276,
     "end_time": "2025-08-04T21:58:57.037447",
     "exception": false,
     "start_time": "2025-08-04T21:58:57.033171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Monografía Final\n",
    "## Redes Neuronales - FIUBA\n",
    "### Alumno: Julián Stejman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96176c8",
   "metadata": {
    "papermill": {
     "duration": 0.003216,
     "end_time": "2025-08-04T21:58:57.044353",
     "exception": false,
     "start_time": "2025-08-04T21:58:57.041137",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "En este documento se utilizará una red neuronal entrenada con un algoritmo de aprendizaje por refuerzo PPO (Proximal Policy Optimization) que aprendiese a jugar niveles de Super Mario Bros para la NES. Para ello, se utilizará el entorno de OpenAI Gym, que simula el juego de Super Mario Bros, junto con LSTM para capturar dependencias temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "329f51d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T21:58:57.052342Z",
     "iopub.status.busy": "2025-08-04T21:58:57.052043Z",
     "iopub.status.idle": "2025-08-04T21:59:14.978369Z",
     "shell.execute_reply": "2025-08-04T21:59:14.977669Z"
    },
    "papermill": {
     "duration": 17.932033,
     "end_time": "2025-08-04T21:59:14.979950",
     "exception": false,
     "start_time": "2025-08-04T21:58:57.047917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym_super_mario_bros in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (7.4.0)\n",
      "Requirement already satisfied: nes-py>=8.1.4 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from gym_super_mario_bros) (8.2.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (1.24.4)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (1.5.21)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (4.67.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym_super_mario_bros) (3.1.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym_super_mario_bros) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym_super_mario_bros) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/julian/.pyenv/versions/3.8.7/envs/Monografia/lib/python3.8/site-packages (from importlib-metadata>=4.10.0->gym>=0.17.2->nes-py>=8.1.4->gym_super_mario_bros) (3.20.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d577e414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T21:59:14.989717Z",
     "iopub.status.busy": "2025-08-04T21:59:14.989356Z",
     "iopub.status.idle": "2025-08-04T21:59:14.993504Z",
     "shell.execute_reply": "2025-08-04T21:59:14.992712Z"
    },
    "papermill": {
     "duration": 0.010306,
     "end_time": "2025-08-04T21:59:14.994886",
     "exception": false,
     "start_time": "2025-08-04T21:59:14.984580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdbedbb",
   "metadata": {
    "papermill": {
     "duration": 0.004009,
     "end_time": "2025-08-04T21:59:15.003077",
     "exception": false,
     "start_time": "2025-08-04T21:59:14.999068",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Si se eliminara en la siguiente celda la línea que dice %%skip, se podría ver una simulación simple de como funciona el emulador del juego con el entorno. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3cf543c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T21:59:15.011905Z",
     "iopub.status.busy": "2025-08-04T21:59:15.011693Z",
     "iopub.status.idle": "2025-08-04T21:59:15.015066Z",
     "shell.execute_reply": "2025-08-04T21:59:15.014429Z"
    },
    "papermill": {
     "duration": 0.009066,
     "end_time": "2025-08-04T21:59:15.016195",
     "exception": false,
     "start_time": "2025-08-04T21:59:15.007129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "%%skip\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "done = True\n",
    "for step in range(5000):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c728c06",
   "metadata": {
    "papermill": {
     "duration": 0.003926,
     "end_time": "2025-08-04T21:59:15.024275",
     "exception": false,
     "start_time": "2025-08-04T21:59:15.020349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "La idea sería armar una red neuronal convolucional para poder interpretar de las imágenes recibidas una posible acción para que tomara el personaje del juego. Esta red luego utilizará un algoritmo de aprendizaje por refuerzo para poder mejorar su desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d6fa22",
   "metadata": {
    "papermill": {
     "duration": 0.003892,
     "end_time": "2025-08-04T21:59:15.032244",
     "exception": false,
     "start_time": "2025-08-04T21:59:15.028352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "La arquitectura utiliza una red Actor-Critic que procesa las imágenes del juego a través de capas convolucionales, seguidas por una LSTM para capturar dependencias temporales. El Actor predice la distribución de probabilidades sobre acciones (policy), mientras que el Critic estima el valor esperado del estado para el cálculo del advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "16819da2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T21:59:15.040974Z",
     "iopub.status.busy": "2025-08-04T21:59:15.040766Z",
     "iopub.status.idle": "2025-08-04T21:59:15.046320Z",
     "shell.execute_reply": "2025-08-04T21:59:15.045688Z"
    },
    "papermill": {
     "duration": 0.011151,
     "end_time": "2025-08-04T21:59:15.047447",
     "exception": false,
     "start_time": "2025-08-04T21:59:15.036296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PPOActorCritic(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions, hidden_size=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Capas convolucionales para procesamiento de imágenes\n",
    "        self.image_processing = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Calcular tamaño de salida de las conv\n",
    "        flatten_size = self._get_flatten_size(input_shape)\n",
    "        \n",
    "        # LSTM para dependencias temporales\n",
    "        self.lstm = nn.LSTM(flatten_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Capas Actor y Critic\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_actions)\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def _get_flatten_size(self, input_shape):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)\n",
    "            output = self.image_processing(dummy_input)\n",
    "            return int(output.view(1, -1).size(1))\n",
    "    \n",
    "    def forward(self, x, hidden_state=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, channels, height, width)\n",
    "        hidden_state: (h, c) tuple de LSTM o None\n",
    "        \"\"\"\n",
    "        # Procesar imágenes\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "        x = x.view(batch_size * seq_len, *x.shape[2:])\n",
    "        x = self.image_processing(x)\n",
    "        x = x.view(batch_size * seq_len, -1)\n",
    "        x = x.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # LSTM\n",
    "        if hidden_state is None:\n",
    "            x, hidden_state = self.lstm(x)\n",
    "        else:\n",
    "            x, hidden_state = self.lstm(x, hidden_state)\n",
    "        \n",
    "        # Usar solo el último output\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        # Actor y Critic\n",
    "        action_logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        \n",
    "        return action_logits, value, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "74c2e842",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T21:59:15.056459Z",
     "iopub.status.busy": "2025-08-04T21:59:15.056265Z",
     "iopub.status.idle": "2025-08-04T21:59:15.264907Z",
     "shell.execute_reply": "2025-08-04T21:59:15.263996Z"
    },
    "papermill": {
     "duration": 0.214476,
     "end_time": "2025-08-04T21:59:15.266192",
     "exception": false,
     "start_time": "2025-08-04T21:59:15.051716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action logits shape: torch.Size([1, 5])\n",
      "Value shape: torch.Size([1, 1])\n",
      "Number of actions: 5\n"
     ]
    }
   ],
   "source": [
    "frame_stack_count = 4\n",
    "dimensions = 84\n",
    "input_shape = (frame_stack_count, dimensions, dimensions)\n",
    "num_actions = len(RIGHT_ONLY)\n",
    "\n",
    "test_model = PPOActorCritic(input_shape, num_actions)\n",
    "dummy_state = torch.zeros(1, 1, *input_shape)  # (batch_size, seq_len, channels, height, width)\n",
    "action_logits, value, hidden_state = test_model(dummy_state)\n",
    "print(f\"Action logits shape: {action_logits.shape}\")\n",
    "print(f\"Value shape: {value.shape}\")\n",
    "print(f\"Number of actions: {num_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbd1900",
   "metadata": {
    "papermill": {
     "duration": 0.00414,
     "end_time": "2025-08-04T21:59:15.274790",
     "exception": false,
     "start_time": "2025-08-04T21:59:15.270650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Para enviar a la red, se toma la decisión de mandar los colores a blanco y negro,bajarle la resolución a 84x84 que es la que se suele utilizar para este tipo de ensayos, y de devolver en cada estado 4 cuadros. De esta forma la red podrá aprender en cada estado con más información del entorno y menos en definición visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f84b976c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T21:59:15.283848Z",
     "iopub.status.busy": "2025-08-04T21:59:15.283601Z",
     "iopub.status.idle": "2025-08-04T21:59:15.924678Z",
     "shell.execute_reply": "2025-08-04T21:59:15.923738Z"
    },
    "papermill": {
     "duration": 0.647322,
     "end_time": "2025-08-04T21:59:15.926247",
     "exception": false,
     "start_time": "2025-08-04T21:59:15.278925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from gym.wrappers import GrayScaleObservation, FrameStack, ResizeObservation\n",
    "\n",
    "# Create the Super Mario environment\n",
    "env = gym.make('SuperMarioBros-v0')\n",
    "\n",
    "# Apply the wrappers for preprocessing\n",
    "env = GrayScaleObservation(env, keep_dim=True)  # Convert to grayscale (removes color channels)\n",
    "env = ResizeObservation(env, shape=(dimensions,dimensions))     # Resize to 84x84\n",
    "env = FrameStack(env, num_stack=frame_stack_count)               # Stack 4 frames\n",
    "\n",
    "# Reset the environment to start\n",
    "state = env.reset()\n",
    "\n",
    "# Visualize the preprocessed frame (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(state[0], cmap='gray')  # Show the first frame in the stack\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483aeca5",
   "metadata": {
    "papermill": {
     "duration": 0.00482,
     "end_time": "2025-08-04T21:59:15.936235",
     "exception": false,
     "start_time": "2025-08-04T21:59:15.931415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Para el entrenamiento con PPO, se utiliza un rollout buffer que acumula experiencias de un episodio completo o de N pasos de interacción. A diferencia de DQN (off-policy), PPO es on-policy: utiliza las experiencias para actualizar la red y luego las descarta, enfocándose en lo más reciente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0680f9f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T21:59:15.946984Z",
     "iopub.status.busy": "2025-08-04T21:59:15.946696Z",
     "iopub.status.idle": "2025-08-04T21:59:15.951298Z",
     "shell.execute_reply": "2025-08-04T21:59:15.950620Z"
    },
    "papermill": {
     "duration": 0.011352,
     "end_time": "2025-08-04T21:59:15.952437",
     "exception": false,
     "start_time": "2025-08-04T21:59:15.941085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_state(state):\n",
    "    \"\"\"\n",
    "    Preprocesa el estado del juego:\n",
    "    - Convierte a escala de grises\n",
    "    - Normaliza entre 0-1\n",
    "    - Redimensiona a 84x84\n",
    "    \"\"\"\n",
    "    # Hacer copia para evitar problemas con strides negativos\n",
    "    if isinstance(state, np.ndarray):\n",
    "        state = state.copy()\n",
    "    \n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    \n",
    "    # Si está en RGB (3 canales), convertir a escala de grises\n",
    "    if len(state.shape) == 3:\n",
    "        # Media ponderada: luminancia = 0.299*R + 0.587*G + 0.114*B\n",
    "        state = 0.299 * state[:,:,0] + 0.587 * state[:,:,1] + 0.114 * state[:,:,2]\n",
    "    \n",
    "    # Normalizar a [0, 1]\n",
    "    state = state / 255.0\n",
    "    \n",
    "    # Redimensionar a 84x84 si es necesario\n",
    "    if state.shape != (84, 84):\n",
    "        import torch.nn.functional as F\n",
    "        state = F.interpolate(state.unsqueeze(0).unsqueeze(0), size=(84, 84), mode='bilinear', align_corners=False)\n",
    "        state = state.squeeze(0).squeeze(0)\n",
    "    \n",
    "    return state\n",
    "\n",
    "class CustomFrameStack:\n",
    "    \"\"\"\n",
    "    Mantiene un stack de los últimos k frames.\n",
    "    Necesario porque LSTM necesita secuencias temporales para capturar dependencias.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=4):\n",
    "        self.k = k\n",
    "        self.frames = deque(maxlen=k)\n",
    "    \n",
    "    def reset(self, state):\n",
    "        \"\"\"Resetea el stack e inicializa con el nuevo estado\"\"\"\n",
    "        state = preprocess_state(state)\n",
    "        self.frames.clear()\n",
    "        # Llenar con el mismo frame inicial\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(state)\n",
    "        return self.get_state()\n",
    "    \n",
    "    def push(self, state):\n",
    "        \"\"\"Agrega un nuevo frame al stack\"\"\"\n",
    "        state = preprocess_state(state)\n",
    "        self.frames.append(state)\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"Retorna el stack actual como (k, 84, 84)\"\"\"\n",
    "        return torch.stack(list(self.frames))\n",
    "\n",
    "class RolloutBuffer:\n",
    "    \"\"\"\n",
    "    Buffer on-policy para PPO: almacena experiencias hasta end_episode()\n",
    "    \n",
    "    A diferencia de DQN, PPO es on-policy: recolecta experiencias, \n",
    "    las usa para actualizar, y luego las descarta.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def push(self, state, action, reward, value, log_prob, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def compute_returns_and_advantages(self, gamma=0.99, gae_lambda=0.95):\n",
    "        \"\"\"\n",
    "        Calcula returns y advantages usando Generalized Advantage Estimation (GAE)\n",
    "        \n",
    "        GAE mezcla TD de corto plazo con Monte Carlo de largo plazo.\n",
    "        λ = 0.95 da buen balance entre bias y varianza.\n",
    "        \"\"\"\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        next_value = 0\n",
    "        \n",
    "        # Iterar hacia atrás a través de los rewards\n",
    "        for t in reversed(range(len(self.rewards))):\n",
    "            if t == len(self.rewards) - 1:\n",
    "                next_non_terminal = 1.0 - self.dones[t]\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - self.dones[t]\n",
    "                next_value = self.values[t + 1]\n",
    "            \n",
    "            # δ_t = r_t + γ*V(s_{t+1}) - V(s_t)\n",
    "            delta = self.rewards[t] + gamma * next_value * next_non_terminal - self.values[t]\n",
    "            # gae_t = δ_t + (γλ) * gae_{t+1}\n",
    "            gae = delta + gamma * gae_lambda * next_non_terminal * gae\n",
    "            \n",
    "            returns.insert(0, gae + self.values[t])\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        return torch.tensor(returns, dtype=torch.float32), torch.tensor(advantages, dtype=torch.float32)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.dones = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8c294",
   "metadata": {
    "papermill": {
     "duration": 0.004431,
     "end_time": "2025-08-04T21:59:15.961562",
     "exception": false,
     "start_time": "2025-08-04T21:59:15.957131",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Función de Entrenamiento PPO Corregida\n",
    "\n",
    "### Cambios principales:\n",
    "\n",
    "**1. FrameStack integrado**\n",
    "- Ahora sí usamos 4 frames: `(4, 84, 84)` para cada estado\n",
    "- La LSTM recibe secuencias con información temporal real\n",
    "\n",
    "**2. Dimensiones correctas**\n",
    "- Estado individual: `(4, 84, 84)`\n",
    "- Estado batch: `(batch_size, 1, 4, 84, 84)` donde `1` es la secuencia de longitud 1\n",
    "- La LSTM procesa esta dimensión de secuencia\n",
    "\n",
    "**3. Hidden state persistente**\n",
    "- El `hidden_state` de LSTM se mantiene entre pasos dentro del mismo episodio\n",
    "- Permite que la LSTM recuerde información del pasado\n",
    "\n",
    "**4. Mejor logging**\n",
    "- Mostramos reward del episodio, promedio de últimos 10, y cantidad de pasos\n",
    "- Checkpoints automáticos cada N episodios\n",
    "\n",
    "### Flujo del entrenamiento:\n",
    "```\n",
    "Para cada episodio:\n",
    "  1. Recolectar experiencias (rollout)\n",
    "     - Forward pass en modelo\n",
    "     - Sample acción y guardar (state, action, reward, log_prob, value)\n",
    "  2. Calcular returns y advantages con GAE\n",
    "  3. Normalizar advantages\n",
    "  4. Para cada época (4 típicamente):\n",
    "     - Pasar por mini-batches\n",
    "     - Calcular PPO loss (clipped objective)\n",
    "     - Actualizar modelo\n",
    "  5. Guardar checkpoint cada 10 episodios\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2800685f",
   "metadata": {
    "papermill": {
     "duration": 0.004488,
     "end_time": "2025-08-04T21:59:15.970642",
     "exception": false,
     "start_time": "2025-08-04T21:59:15.966154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Entrenamiento PPO\n",
    "\n",
    "El algoritmo PPO entrena la red actor-critic utilizando rollouts de experiencias recientes. Cada cierto número de steps, se realiza una actualización con múltiples épocas sobre el mismo batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "807f8a31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T21:59:15.980837Z",
     "iopub.status.busy": "2025-08-04T21:59:15.980607Z",
     "iopub.status.idle": "2025-08-04T21:59:15.991115Z",
     "shell.execute_reply": "2025-08-04T21:59:15.990480Z"
    },
    "papermill": {
     "duration": 0.016942,
     "end_time": "2025-08-04T21:59:15.992206",
     "exception": false,
     "start_time": "2025-08-04T21:59:15.975264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_ppo(env, model, num_episodes=100, rollout_steps=2048, num_epochs=4,\n",
    "              batch_size=64, gamma=0.99, gae_lambda=0.95, learning_rate=3e-4,\n",
    "              clip_ratio=0.2, entropy_coef=0.01, value_coef=0.5,\n",
    "              device=\"cpu\", checkpoint_freq=10, checkpoint_path=\"mario_ppo.pt\"):\n",
    "    \"\"\"\n",
    "    Entrena PPO en el ambiente SuperMario\n",
    "    \n",
    "    El algoritmo:\n",
    "    1. Recolecta experiencias con la política actual\n",
    "    2. Calcula advantages usando GAE\n",
    "    3. Actualiza actor y critic múltiples épocas sobre los mismos datos\n",
    "    4. Usa clipping de PPO para evitar actualizaciones muy grandes\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    total_rewards = []\n",
    "    step_count = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # ========== RECOLECCIÓN DE EXPERIENCIAS ==========\n",
    "        \n",
    "        state = env.reset()\n",
    "        frame_stack = CustomFrameStack(k=4)\n",
    "        stacked_state = frame_stack.reset(state)\n",
    "        \n",
    "        buffer = RolloutBuffer()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        done = False\n",
    "        hidden_state = None\n",
    "        \n",
    "        print(f\"Episodio {episode + 1}/{num_episodes}\")\n",
    "        \n",
    "        # Variable para reward shaping basado en progreso en X\n",
    "        prev_x_pos = 40  # Posición inicial aproximada de Mario\n",
    "        \n",
    "        # Recolectar experiencias para este episodio\n",
    "        while not done:  # Sin límite de pasos\n",
    "            # Convertir estado a batch\n",
    "            state_batch = stacked_state.unsqueeze(0).unsqueeze(0).to(device)  # (1, 1, 4, 84, 84)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                action_logits, value, hidden_state = model(state_batch, hidden_state)\n",
    "                probs = torch.softmax(action_logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                \n",
    "                # Epsilon-greedy mejorado: más exploración y decaimiento más lento\n",
    "                epsilon = max(0.05, 0.3 - episode * 0.003)  # Decae de 0.3 a 0.05\n",
    "                if np.random.random() < epsilon:\n",
    "                    action = torch.tensor(env.action_space.sample())\n",
    "                else:\n",
    "                    action = dist.sample()\n",
    "                \n",
    "                log_prob = dist.log_prob(action)\n",
    "            \n",
    "            # Interaction with environment - compatible con gym viejo y nuevo\n",
    "            step_result = env.step(action.item())\n",
    "            if len(step_result) == 5:  # gym >= 0.26\n",
    "                next_state, reward, terminated, truncated, info = step_result\n",
    "                done = terminated or truncated\n",
    "            else:  # gym < 0.26\n",
    "                next_state, reward, done, info = step_result\n",
    "            \n",
    "            # Reward shaping: Recompensar por avanzar en X\n",
    "            current_x_pos = info.get('x_pos', prev_x_pos)\n",
    "            x_progress = current_x_pos - prev_x_pos\n",
    "            reward += x_progress * 0.1  # Bonus por avanzar a la derecha\n",
    "            prev_x_pos = current_x_pos\n",
    "            \n",
    "            # Stack frames\n",
    "            next_stacked_state = frame_stack.push(next_state)\n",
    "            \n",
    "            # Store in buffer\n",
    "            buffer.push(\n",
    "                stacked_state.cpu().numpy(),  # Estado original (4, 84, 84)\n",
    "                action.item(),\n",
    "                reward,\n",
    "                value.item(),\n",
    "                log_prob.item(),\n",
    "                float(done)\n",
    "            )\n",
    "            \n",
    "            episode_reward += reward\n",
    "            stacked_state = next_stacked_state\n",
    "            step_count += 1\n",
    "            episode_length += 1\n",
    "        \n",
    "        # ========== CÁLCULO DE RETURNS Y ADVANTAGES ==========\n",
    "        # Convertir buffer a tensores\n",
    "        returns, advantages = buffer.compute_returns_and_advantages(gamma, gae_lambda)\n",
    "        \n",
    "        # Convertir buffer a tensores\n",
    "        states_array = np.array(buffer.states)  # (T, 4, 84, 84)\n",
    "        states_tensor = torch.tensor(states_array, dtype=torch.float32).to(device)\n",
    "        \n",
    "        actions_tensor = torch.tensor(buffer.actions, dtype=torch.long).to(device)\n",
    "        old_log_probs = torch.tensor(buffer.log_probs, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Normalizar advantages (importante para estabilidad)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        advantages = advantages.to(device)\n",
    "        returns = returns.to(device)\n",
    "        \n",
    "        # ========== ACTUALIZACIÓN PPO ==========\n",
    "        \n",
    "        num_batches = max(1, len(buffer.states) // batch_size)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffled minibatch updates\n",
    "            indices = torch.randperm(len(buffer.states))\n",
    "            \n",
    "            for batch_idx in range(num_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = min(start_idx + batch_size, len(buffer.states))\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_states = states_tensor[batch_indices]  # (B, 4, 84, 84)\n",
    "                batch_actions = actions_tensor[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                \n",
    "                # Agregar dimensión de secuencia para LSTM: (B, 1, 4, 84, 84)\n",
    "                batch_states = batch_states.unsqueeze(1)\n",
    "                \n",
    "                # Forward pass\n",
    "                action_logits, values, _ = model(batch_states)\n",
    "                \n",
    "                # Calcular nuevos log probs\n",
    "                probs = torch.softmax(action_logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                new_log_probs = dist.log_prob(batch_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # ===== PPO LOSS =====\n",
    "                # Ratio de probabilidades: π_new / π_old\n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                \n",
    "                # Clipped objective (PPO's key innovation)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1.0 - clip_ratio, 1.0 + clip_ratio) * batch_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # ===== VALUE LOSS =====\n",
    "                value_loss = ((values.squeeze() - batch_returns) ** 2).mean()\n",
    "                \n",
    "                # ===== TOTAL LOSS =====\n",
    "                # Combinamos: actor loss + value loss - entropy (para exploración)\n",
    "                total_loss = actor_loss + value_coef * value_loss - entropy_coef * entropy\n",
    "                \n",
    "                # Optimization step\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Agregar reward del episodio a la lista\n",
    "        total_rewards.append(episode_reward)\n",
    "        \n",
    "        # Calcular y mostrar estadísticas\n",
    "        avg_10 = np.mean(total_rewards[-10:]) if len(total_rewards) >= 10 else np.mean(total_rewards)\n",
    "        max_reward = max(total_rewards)\n",
    "        print(f\"  Reward: {episode_reward:.1f} | Avg(10): {avg_10:.1f} | Max: {max_reward:.1f} | Steps: {episode_length} | X_pos: {prev_x_pos:.0f} | Epsilon: {epsilon:.3f}\")\n",
    "        \n",
    "        # Checkpoint\n",
    "        if (episode + 1) % checkpoint_freq == 0:\n",
    "            save_checkpoint(model, total_rewards, checkpoint_path, device)\n",
    "            print(f\"  📊 Progreso: Mejor={max_reward:.1f}, Promedio últimos 10={avg_10:.1f}\")\n",
    "    \n",
    "    return total_rewards\n",
    "\n",
    "def save_checkpoint(model, total_rewards, checkpoint_path, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Guarda el modelo y training state con torch.save()\n",
    "    \n",
    "    torch.save es mejor que pickle:\n",
    "    - Versión-agnóstico\n",
    "    - Mejor performance\n",
    "    - Standard en la comunidad de PyTorch\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'total_rewards': total_rewards,\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"✓ Checkpoint guardado en {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(model, checkpoint_path, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Carga modelo y training state con torch.load()\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    total_rewards = checkpoint['total_rewards']\n",
    "    print(f\"✓ Checkpoint cargado desde {checkpoint_path}\")\n",
    "    return model, total_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7ef116",
   "metadata": {
    "papermill": {
     "duration": 0.004332,
     "end_time": "2025-08-04T21:59:16.001170",
     "exception": false,
     "start_time": "2025-08-04T21:59:15.996838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocesamiento y Frame Stacking\n",
    "\n",
    "### Preprocesamiento (`preprocess_state`)\n",
    "Cada frame del juego se procesa:\n",
    "1. **Conversión a escala de grises**: Reduce de 3 canales (RGB) a 1. Usamos luminancia estándar: `0.299*R + 0.587*G + 0.114*B`\n",
    "2. **Normalización**: Divide entre 255 para llevar valores a [0, 1]\n",
    "3. **Redimensionamiento**: Convierte a 84×84 (tamaño estándar en RL)\n",
    "\n",
    "**Ventajas**: Menos memoria, entrenamiento más rápido, sin perder información importante.\n",
    "\n",
    "### Frame Stacking (`CustomFrameStack`)\n",
    "¿Por qué necesitamos 4 frames?\n",
    "- **Un frame no contiene información de velocidad**: No sabemos si Mario se mueve izquierda, derecha o está parado\n",
    "- **LSTM necesita secuencias**: Sin múltiples frames, la LSTM no puede aprender dependencias temporales\n",
    "- **4 frames es estándar**: Es suficiente para capturar movimiento y velocidad\n",
    "\n",
    "El stack siempre tiene exactamente 4 frames: `(4, 84, 84)`\n",
    "\n",
    "**Nota**: Usamos `CustomFrameStack` para nuestro preprocesamiento interno, y `gym.wrappers.FrameStack` para el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "07d3c86d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T21:59:16.011105Z",
     "iopub.status.busy": "2025-08-04T21:59:16.010898Z",
     "iopub.status.idle": "2025-08-04T21:59:16.015319Z",
     "shell.execute_reply": "2025-08-04T21:59:16.014706Z"
    },
    "papermill": {
     "duration": 0.010712,
     "end_time": "2025-08-04T21:59:16.016498",
     "exception": false,
     "start_time": "2025-08-04T21:59:16.005786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dispositivo a usar: cpu\n",
      "ⓘ No se encontró checkpoint anterior. Comenzando entrenamiento desde cero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20531/3653937601.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=== CONFIGURACIÓN INICIAL DEL ENTRENAMIENTO ===\n",
    "\n",
    "Aquí inicializamos:\n",
    "- El dispositivo (GPU si está disponible)\n",
    "- El modelo PPO con arquitectura Actor-Critic + LSTM\n",
    "- Intentamos cargar un checkpoint anterior si existe\n",
    "\"\"\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"✓ Dispositivo a usar: {device}\")\n",
    "\n",
    "# Inicializar modelo\n",
    "model = PPOActorCritic(input_shape, num_actions, hidden_size=128)\n",
    "\n",
    "# Intentar cargar checkpoint anterior para continuar entrenamiento\n",
    "checkpoint_path = \"mario_ppo_checkpoint.pt\"\n",
    "try:\n",
    "    model, prev_rewards = load_checkpoint(model, checkpoint_path, device)\n",
    "    print(f\"✓ Reanudando entrenamiento con {len(prev_rewards)} episodios previos\")\n",
    "    print(f\"  Mejor reward: {max(prev_rewards):.2f}\")\n",
    "    print(f\"  Último reward: {prev_rewards[-1]:.2f}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ⓘ No se encontró checkpoint anterior. Comenzando entrenamiento desde cero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f148e51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T21:59:16.026349Z",
     "iopub.status.busy": "2025-08-04T21:59:16.026156Z",
     "iopub.status.idle": "2025-08-05T01:02:28.628687Z",
     "shell.execute_reply": "2025-08-05T01:02:28.627922Z"
    },
    "papermill": {
     "duration": 10992.609191,
     "end_time": "2025-08-05T01:02:28.630318",
     "exception": false,
     "start_time": "2025-08-04T21:59:16.021127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VERIFICACIÓN DEL AMBIENTE ===\n",
      "✓ Estado shape: (240, 256, 3), dtype: uint8\n",
      "✓ env.step() devuelve 4 valores\n",
      "✓ Action space: 5 acciones\n",
      "=== FIN VERIFICACIÓN ===\n",
      "\n",
      "✓ Entorno configurado correctamente (sin wrappers de preprocesamiento)\n",
      "Episodio 1/100\n",
      "  Reward: 557.4 | Avg(10): 557.4 | Max: 557.4 | Steps: 24062 | X_pos: 594 | Epsilon: 0.300\n",
      "Episodio 2/100\n"
     ]
    }
   ],
   "source": [
    "# Imports necesarios\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configurar entorno con todos los wrappers necesarios\n",
    "# Nota: NO usamos FrameStack aquí porque CustomFrameStack lo maneja internamente\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n",
    "\n",
    "# Verificar ambiente\n",
    "print(\"\\n=== VERIFICACIÓN DEL AMBIENTE ===\")\n",
    "test_state = env.reset()\n",
    "print(f\"✓ Estado shape: {test_state.shape}, dtype: {test_state.dtype}\")\n",
    "test_result = env.step(0)\n",
    "print(f\"✓ env.step() devuelve {len(test_result)} valores\")\n",
    "print(f\"✓ Action space: {env.action_space.n} acciones\")\n",
    "print(\"=== FIN VERIFICACIÓN ===\\n\")\n",
    "\n",
    "print(\"✓ Entorno configurado correctamente (sin wrappers de preprocesamiento)\")\n",
    "\n",
    "# Entrenamiento\n",
    "num_episodes = 100\n",
    "total_rewards = train_ppo(\n",
    "    env,\n",
    "    model,\n",
    "    num_episodes=num_episodes,\n",
    "    rollout_steps=2048,\n",
    "    num_epochs=4,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    learning_rate=3e-4,\n",
    "    clip_ratio=0.2,\n",
    "    entropy_coef=0.05,  # Aumentado de 0.01 a 0.05 para más exploración\n",
    "    value_coef=0.5,\n",
    "    device=device,\n",
    "    checkpoint_freq=10,\n",
    "    checkpoint_path=\"mario_ppo_checkpoint.pt\"\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(total_rewards, label='Episode Reward')\n",
    "plt.plot(np.convolve(total_rewards, np.ones(10)/10, mode='valid'), label='Avg (window=10)', linewidth=2)\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "print(f\"Last 10 episodes avg: {np.mean(total_rewards[-10:]):.2f}\")\n",
    "\n",
    "plt.ylabel('Reward')\n",
    "print(f\"Max reward: {max(total_rewards)}\")\n",
    "\n",
    "plt.title('PPO Training on Super Mario Bros')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878fb460",
   "metadata": {},
   "source": [
    "## Inicio del Entrenamiento\n",
    "\n",
    "A continuación entrenaremos el modelo con PPO. Parámetros clave:\n",
    "- **num_episodes=100**: 100 episodios (ajusta según necesidad)\n",
    "- **rollout_steps=2048**: Recolectar ~2000 pasos antes de actualizar\n",
    "- **num_epochs=4**: Reusar datos 4 veces para cada actualización\n",
    "- **batch_size=64**: Tamaño de mini-batch\n",
    "- **clip_ratio=0.2**: PPO clipping (mantiene cambios de política pequeños)\n",
    "\n",
    "**En Kaggle**: Si tienes tiempo limitado, puedes reducir `num_episodes` o `rollout_steps`.\n",
    "\n",
    "El modelo guardará checkpoints automáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f5c54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T01:02:28.719948Z",
     "iopub.status.busy": "2025-08-05T01:02:28.719507Z",
     "iopub.status.idle": "2025-08-05T01:02:28.724047Z",
     "shell.execute_reply": "2025-08-05T01:02:28.723112Z"
    },
    "papermill": {
     "duration": 0.049982,
     "end_time": "2025-08-05T01:02:28.725257",
     "exception": false,
     "start_time": "2025-08-05T01:02:28.675275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing trained model...\n",
      "Ejecutando 5 episodios de prueba...\n",
      "\n",
      "Episodio 1: Reward =  680.00 | Steps =  481\n",
      "Episodio 2: Reward =  680.00 | Steps =  481\n",
      "Episodio 3: Reward =  680.00 | Steps =  481\n",
      "Episodio 4: Reward =  680.00 | Steps =  481\n",
      "Episodio 5: Reward =  680.00 | Steps =  481\n",
      "\n",
      "==================================================\n",
      "Average reward: 680.00 ± 0.00\n",
      "Max reward: 680.00\n",
      "Min reward: 680.00\n",
      "Average steps: 481\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=== INFERENCIA: USAR EL MODELO ENTRENADO ===\n",
    "\n",
    "Aquí jugamos episodios usando la política aprendida.\n",
    "Usamos acciones deterministas (argmax) para aprovechar lo aprendido.\n",
    "\"\"\"\n",
    "\n",
    "def play_episode(env, model, device=\"cpu\", max_steps=1000):\n",
    "    \"\"\"\n",
    "    Juega un episodio usando la política aprendida\n",
    "    \n",
    "    Args:\n",
    "        env: Ambiente SuperMario\n",
    "        model: Modelo PPO Actor-Critic\n",
    "        device: CPU o CUDA\n",
    "        max_steps: Máximo de pasos por episodio\n",
    "    \n",
    "    Returns:\n",
    "        total_reward: Recompensa total acumulada\n",
    "        steps: Número de pasos ejecutados\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Inicializar frame stack\n",
    "    frame_stack = CustomFrameStack(k=4)\n",
    "    stacked_state = frame_stack.reset(state)\n",
    "    \n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    hidden_state = None\n",
    "    \n",
    "    while not done and steps < max_steps:\n",
    "        # Convertir a batch: (1, 1, 4, 84, 84)\n",
    "        state_batch = stacked_state.unsqueeze(0).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_logits, _, hidden_state = model(state_batch, hidden_state)\n",
    "            # Usar acción determinista (greedy): la de mayor probabilidad\n",
    "            action = action_logits.argmax(dim=-1)\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action.item())\n",
    "        \n",
    "        # Stack frames\n",
    "        stacked_state = frame_stack.push(next_state)\n",
    "        \n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "    \n",
    "    model.train()\n",
    "    return total_reward, steps\n",
    "\n",
    "# ===== TESTING =====\n",
    "print(\"Testing trained model...\")\n",
    "print(\"Ejecutando 5 episodios de prueba...\\n\")\n",
    "\n",
    "test_rewards = []\n",
    "test_steps = []\n",
    "\n",
    "for i in range(5):\n",
    "    reward, steps = play_episode(env, model, device=device, max_steps=1000)\n",
    "    test_rewards.append(reward)\n",
    "    test_steps.append(steps)\n",
    "    print(f\"Episodio {i+1}: Reward = {reward:7.2f} | Steps = {steps:4d}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Average reward: {np.mean(test_rewards):.2f} ± {np.std(test_rewards):.2f}\")\n",
    "print(f\"Max reward: {max(test_rewards):.2f}\")\n",
    "print(f\"Min reward: {min(test_rewards):.2f}\")\n",
    "print(f\"Average steps: {np.mean(test_steps):.0f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db4b1d",
   "metadata": {
    "papermill": {
     "duration": 0.046319,
     "end_time": "2025-08-05T01:02:28.815211",
     "exception": false,
     "start_time": "2025-08-05T01:02:28.768892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Conclusiones y Mejoras Implementadas\n",
    "\n",
    "### ✅ Correcciones realizadas:\n",
    "\n",
    "1. **Preprocesamiento de imágenes**\n",
    "   - Convertir a escala de grises\n",
    "   - Normalizar a [0, 1]\n",
    "   - Redimensionar a 84×84\n",
    "\n",
    "2. **Frame Stacking correcto**\n",
    "   - 4 frames apilados: `(4, 84, 84)`\n",
    "   - La LSTM realmente recibe secuencias temporales\n",
    "\n",
    "3. **Dimensiones fijas**\n",
    "   - Estado: `(4, 84, 84)`\n",
    "   - Batch de estados: `(B, 1, 4, 84, 84)` para LSTM\n",
    "\n",
    "4. **Hidden state persistente**\n",
    "   - Se mantiene entre pasos del episodio\n",
    "   - Permite memoria a través de la secuencia\n",
    "\n",
    "5. **Mejor logging y checkpoints**\n",
    "   - Información clara del progreso\n",
    "   - Guardado automático cada N episodios\n",
    "\n",
    "### 🎯 Ventajas PPO + LSTM vs DQN:\n",
    "- **Convergencia más rápida**: PPO necesita menos samples que DQN\n",
    "- **Más estable**: Policy gradient + clipping evita cambios abruptos\n",
    "- **Memoria temporal**: LSTM captura dependencias, no solo estado actual\n",
    "- **On-policy**: Usa datos recientes y relevantes\n",
    "\n",
    "### 📊 Esperado en entrenamiento:\n",
    "- Primeros episodios: rewards muy bajos (0-500)\n",
    "- Episodios 20-50: Empieza a aprender (500-2000)\n",
    "- Episodios 50+: Debería mejorar significativamente (>2000)\n",
    "\n",
    "### 💾 Checkpoint system:\n",
    "- Automático cada 10 episodios\n",
    "- Puede reanudar entrenamiento sin perder progreso\n",
    "- Archivo: `mario_ppo_checkpoint.pt`"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Monografia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11016.192569,
   "end_time": "2025-08-05T01:02:30.594241",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-04T21:58:54.401672",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
